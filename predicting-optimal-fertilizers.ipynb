{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91717,"databundleVersionId":12184666,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":12333826,"sourceType":"datasetVersion","datasetId":7774967}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-30T07:23:49.959161Z","iopub.execute_input":"2025-06-30T07:23:49.959547Z","iopub.status.idle":"2025-06-30T07:23:49.968356Z","shell.execute_reply.started":"2025-06-30T07:23:49.959517Z","shell.execute_reply":"2025-06-30T07:23:49.967267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\ndf_train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\ndf_train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# null values\ndf_train.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# column types\nnumerical_cols = df_train.select_dtypes(include=['int64', 'int32']).columns.tolist()\ncategorical_cols = df_train.select_dtypes(include=['object', 'category']).columns.tolist()\n\nprint(\"numerical Columns:\", numerical_cols)\nprint(\"Categorical Columns:\", categorical_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # plot all the graphs and plots for the dataset to see how the data is organised\n\n# # Histograms for numerical columns\n# for col in numerical_cols:\n#     plt.figure(figsize=(8, 6))\n#     sns.histplot(data=df_train, x=col, kde=True)\n#     plt.title(f'Histogram of {col}')\n#     plt.show()\n\n# # Box plots for numerical columns (useful for visualizing distribution and outliers)\n# for col in numerical_cols:\n#     plt.figure(figsize=(8, 6))\n#     sns.boxplot(data=df_train, y=col)\n#     plt.title(f'Box plot of {col}')\n#     plt.show()\n\n# # # Count plots for categorical columns\n# # categorical_cols = df.select_dtypes(include='object').columns\n# # for col in categorical_cols:\n# #     plt.figure(figsize=(8, 6))\n# #     sns.countplot(data=df, x=col)\n# #     plt.title(f'Count plot of {col}')\n# #     plt.xticks(rotation=45, ha='right')\n# #     plt.show()\n\n# # Pairplot (can be computationally intensive for large datasets)\n# # sns.pairplot(df)\n# # plt.title('Pairwise plots of numerical columns')\n# # plt.show()\n\n# # Correlation heatmap for numerical columns\n# if len(numerical_cols) > 1:\n#     plt.figure(figsize=(10, 8))\n#     sns.heatmap(df_train[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n#     plt.title('Correlation Heatmap of Numerical Columns')\n#     plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LightGBM + KMeans + Class Balancing (Optuna-tuned)","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from sklearn.cluster import KMeans\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.metrics import log_loss\n# import lightgbm as lgb\n# import optuna\n\n# # Load data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix typo\n# train.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n\n# # Encode target\n# le = LabelEncoder()\n# train['target'] = le.fit_transform(train['Fertilizer Name'])\n# n_classes = train['target'].nunique()\n\n# # Target mean encoding\n# for col in ['Soil Type', 'Crop Type']:\n#     mapping = train.groupby(col)['target'].mean()\n#     train[f'{col}_enc'] = train[col].map(mapping)\n#     test[f'{col}_enc'] = test[col].map(mapping)\n\n# features = ['Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous',\n#             'Soil Type_enc','Crop Type_enc']\n# X = train[features].copy()\n# X_test = test[features].copy()\n\n# # Add KMeans cluster\n# kmeans = KMeans(n_clusters=5, random_state=42, n_init='auto')\n# train['cluster'] = kmeans.fit_predict(X)\n# test['cluster'] = kmeans.predict(X_test)\n# X['cluster'] = train['cluster']\n# X_test['cluster'] = test['cluster']\n# y = train['target']\n\n# # Class weights\n# class_counts = y.value_counts().to_dict()\n# total = len(y)\n# weights = {cls: total / cnt for cls, cnt in class_counts.items()}\n# sample_weights = y.map(weights)\n\n# # Optuna objective\n# def objective(trial):\n#     param = {\n#         'objective': 'multiclass',\n#         'num_class': n_classes,\n#         'metric': 'multi_logloss',\n#         'verbosity': -1,\n#         'boosting_type': 'gbdt',\n#         'learning_rate': trial.suggest_float('lr', 0.01, 0.3, log=True),\n#         'num_leaves': trial.suggest_int('num_leaves', 32, 256),\n#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 200),\n#         'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n#         'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n#         'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n#     }\n#     skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n#     losses = []\n#     for train_idx, val_idx in skf.split(X, y):\n#         dtrain = lgb.Dataset(X.iloc[train_idx], label=y.iloc[train_idx], weight=sample_weights.iloc[train_idx])\n#         dval = lgb.Dataset(X.iloc[val_idx], label=y.iloc[val_idx])\n#         gbm = lgb.train(\n#             param, dtrain,\n#             num_boost_round=1000,\n#             valid_sets=[dval],\n#             callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n#         )\n#         preds = gbm.predict(X.iloc[val_idx])\n#         losses.append(log_loss(y.iloc[val_idx], preds))\n#     return np.mean(losses)\n\n# # Optimize\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=40, timeout=600)\n# best_params = study.best_params\n# print(\"✅ Best Params:\", best_params)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T07:23:55.901329Z","iopub.execute_input":"2025-06-30T07:23:55.901672Z","iopub.status.idle":"2025-06-30T07:44:02.285759Z","shell.execute_reply.started":"2025-06-30T07:23:55.901646Z","shell.execute_reply":"2025-06-30T07:44:02.284796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Use optimized parameters\n# best_params.update({\n#     'objective': 'multiclass',\n#     'num_class': n_classes,\n#     'metric': 'multi_logloss',\n#     'verbosity': -1,\n#     'boosting_type': 'gbdt',\n# })\n\n# # Final bagged training\n# from sklearn.model_selection import StratifiedKFold\n\n# BAGS = 5\n# skf = StratifiedKFold(n_splits=BAGS, shuffle=True, random_state=42)\n# test_preds = np.zeros((X_test.shape[0], n_classes))\n\n# for bag, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n#     print(f\"Bag {bag+1}/{BAGS}\")\n#     dtrain = lgb.Dataset(X.iloc[train_idx], label=y.iloc[train_idx], weight=sample_weights.iloc[train_idx])\n#     dval = lgb.Dataset(X.iloc[val_idx], label=y.iloc[val_idx])\n\n#     gbm = lgb.train(\n#         best_params,\n#         dtrain,\n#         num_boost_round=1000,\n#         valid_sets=[dval],\n#         callbacks=[\n#             lgb.early_stopping(50),\n#             lgb.log_evaluation(100)\n#         ]\n#     )\n\n#     test_preds += gbm.predict(X_test) / BAGS\n\n# # Convert predictions to top 3 labels\n# top3 = np.argsort(test_preds, axis=1)[:, -3:][:, ::-1]\n# labels = le.inverse_transform(top3.ravel()).reshape(top3.shape)\n\n# # Submission\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in labels]\n# })\n# submission.to_csv('submission_lgb_kmeans_balance.csv', index=False)\n# print(\"✅ submission_lgb_kmeans_balance.csv saved\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T07:49:50.955555Z","iopub.execute_input":"2025-06-30T07:49:50.956294Z","iopub.status.idle":"2025-06-30T08:01:20.432214Z","shell.execute_reply.started":"2025-06-30T07:49:50.956262Z","shell.execute_reply":"2025-06-30T08:01:20.430895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LightGBM + KMeans cluster features + Class balancing (via sample weighting)","metadata":{}},{"cell_type":"code","source":"# # ✅ Full Kaggle‑Compatible Model: LightGBM + KMeans Features + Class‑Balanced Bagging\n\n# # 🧰 1. Imports\n# import pandas as pd\n# import numpy as np\n# import lightgbm as lgb\n# import optuna\n# from sklearn.preprocessing import LabelEncoder, QuantileTransformer\n# from sklearn.cluster import KMeans\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.metrics import log_loss\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 🧱 2. Load & Preprocess Data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n# train.rename(columns={'Temparature':'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature':'Temperature'}, inplace=True)\n\n# le = LabelEncoder()\n# train['target'] = le.fit_transform(train['Fertilizer Name'])\n# n_classes = train['target'].nunique()\n\n# # Basic numeric features\n# NUM = ['Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous']\n# X = train[NUM].copy()\n# X_test = test[NUM].copy()\n\n# # Target‑mean encoding of categoricals\n# for col in ['Soil Type','Crop Type']:\n#     m = train.groupby(col)['target'].mean()\n#     train[f'{col}_enc'] = train[col].map(m)\n#     test[f'{col}_enc']  = test[col].map(m)\n\n# X['soil_enc'] = train['Soil Type_enc']\n# X['crop_enc'] = train['Crop Type_enc']\n# X_test['soil_enc'] = test['Soil Type_enc']\n# X_test['crop_enc'] = test['Crop Type_enc']\n\n# # Quantile transform to BMI‑style distributed numeric inputs\n# qt = QuantileTransformer(n_quantiles=1000, output_distribution='normal', random_state=42)\n# X[NUM] = qt.fit_transform(X[NUM])\n# X_test[NUM] = qt.transform(X_test[NUM])\n\n# # KMeans clustering on numeric + encoded features\n# kmeans = KMeans(n_clusters=20, random_state=42)\n# X['km']      = kmeans.fit_predict(X)\n# X_test['km'] = kmeans.predict(X_test)\n\n# # Feature list\n# features = X.columns.tolist()\n# X = X.values\n# X_test = X_test.values\n# y = train['target'].values\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 🔍 3. Optuna‑tuned LightGBM Objective\n# def objective(trial):\n#     params = {\n#         'objective': 'multiclass', 'num_class': n_classes,\n#         'metric': 'multi_logloss', 'verbosity': -1, 'boosting_type': 'gbdt',\n#         'learning_rate': trial.suggest_float('lr', 0.02, 0.2, log=True),\n#         'num_leaves': trial.suggest_int('num_leaves', 31, 256),\n#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 200),\n#         'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n#         'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n#         'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n#     }\n#     skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n#     cv_losses = []\n#     for tr_i, val_i in skf.split(X, y):\n#         dtr = lgb.Dataset(X[tr_i], label=y[tr_i])\n#         dval = lgb.Dataset(X[val_i], label=y[val_i])\n#         gbm = lgb.train(params, dtr, num_boost_round=500,\n#                         valid_sets=[dval],\n#                         callbacks=[lgb.early_stopping(stopping_rounds=50)])\n#         cv_losses.append(log_loss(y[val_i], gbm.predict(X[val_i])))\n#     return np.mean(cv_losses)\n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=30, timeout=600)\n# best = study.best_params\n# best.update({'objective':'multiclass','num_class':n_classes, 'metric':'multi_logloss', 'verbosity':-1})\n# print(\"OPTUNA BEST:\", best)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 🎯 4. 5‑fold BAGGED LightGBM\n# BAGS = 5\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# test_preds = np.zeros((len(test), n_classes))\n\n# for bag in range(BAGS):\n#     print(f\"Bag {bag+1}/{BAGS}\")\n#     seed = 5000 + bag\n#     bag_preds = np.zeros_like(test_preds)\n#     for tr_i, val_i in skf.split(X, y):\n#         dtr = lgb.Dataset(X[tr_i], label=y[tr_i])\n#         gbm = lgb.train({**best, 'seed':seed},\n#                         dtr, num_boost_round=best.get('num_boost_round',500))\n#         bag_preds += gbm.predict(X_test) / skf.n_splits\n#     test_preds += bag_preds / BAGS\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 📝 5. Submission\n# top3 = np.argsort(test_preds, axis=1)[:, -3:][:, ::-1]\n# labels = le.inverse_transform(top3.ravel()).reshape(top3.shape)\n# out = pd.DataFrame({'id': test['id'], 'Fertilizer Name': [' '.join(r) for r in labels]})\n# out.to_csv('submission_lgbm_kmeans.csv', index=False)\n# print(\"✅ Submission saved as submission_lgbm_kmeans.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T10:51:58.346731Z","iopub.execute_input":"2025-06-30T10:51:58.347130Z","iopub.status.idle":"2025-06-30T12:03:29.328397Z","shell.execute_reply.started":"2025-06-30T10:51:58.347103Z","shell.execute_reply":"2025-06-30T12:03:29.327323Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dynamic Cluster‑Based Blending,","metadata":{}},{"cell_type":"code","source":"# # ───────────────────────────────────────────────────────────────────────────────\n# # 📦 Install dependencies (run once)\n# !pip install lightgbm xgboost catboost scipy --quiet\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 1️⃣ Imports\n# import pandas as pd\n# import numpy as np\n# from sklearn.preprocessing import LabelEncoder, StandardScaler\n# from sklearn.cluster import KMeans\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.metrics import log_loss\n# import lightgbm as lgb\n# import xgboost as xgb\n# import catboost as cb\n# from scipy.optimize import nnls\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 2️⃣ Load & Prep\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n# train.rename(columns={'Temparature':'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature':'Temperature'}, inplace=True)\n\n# le = LabelEncoder()\n# train['target'] = le.fit_transform(train['Fertilizer Name'])\n# n_classes = train['target'].nunique()\n\n# # Raw numeric features for clustering\n# NUM = ['Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous']\n# X_raw = train[NUM].values\n# X_test_raw = test[NUM].values\n\n# # Scale before clustering\n# scaler = StandardScaler().fit(np.vstack([X_raw, X_test_raw]))\n# X_scaled = scaler.transform(X_raw)\n# X_test_scaled = scaler.transform(X_test_raw)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 3️⃣ Base‑Model OOF & Test Predictions\n# N_SPLITS = 5\n# skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\n# # placeholders\n# oof_lgb = np.zeros((len(train), n_classes))\n# oof_xgb = np.zeros_like(oof_lgb)\n# oof_cb  = np.zeros_like(oof_lgb)\n\n# test_lgb = np.zeros((len(test), n_classes))\n# test_xgb = np.zeros_like(test_lgb)\n# test_cb  = np.zeros_like(test_lgb)\n\n# for fold, (tr, val) in enumerate(skf.split(X_scaled, train['target']), 1):\n#     X_tr, X_val = X_scaled[tr], X_scaled[val]\n#     y_tr, y_val = train['target'].iloc[tr], train['target'].iloc[val]\n\n#     # LightGBM\n#     m1 = lgb.LGBMClassifier(\n#         objective='multiclass', num_class=n_classes,\n#         learning_rate=0.05, n_estimators=500,\n#         random_state=fold\n#     )\n#     m1.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n#            callbacks=[lgb.early_stopping(stopping_rounds=50)])\n#     oof_lgb[val] = m1.predict_proba(X_val)\n#     test_lgb  += m1.predict_proba(X_test_scaled) / N_SPLITS\n\n#     # XGBoost\n#     m2 = xgb.XGBClassifier(\n#         objective='multi:softprob', num_class=n_classes,\n#         learning_rate=0.05, n_estimators=500,\n#         use_label_encoder=False, eval_metric='mlogloss',\n#         random_state=fold\n#     )\n#     m2.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n#            early_stopping_rounds=50, verbose=False)\n#     oof_xgb[val] = m2.predict_proba(X_val)\n#     test_xgb  += m2.predict_proba(X_test_scaled) / N_SPLITS\n\n#     # CatBoost\n#     m3 = cb.CatBoostClassifier(\n#         iterations=500, learning_rate=0.05, depth=6,\n#         loss_function='MultiClass', verbose=0, random_seed=fold\n#     )\n#     m3.fit(X_tr, y_tr)\n#     oof_cb[val] = m3.predict_proba(X_val)\n#     test_cb  += m3.predict_proba(X_test_scaled) / N_SPLITS\n\n# # Stack OOF preds into [n_samples, 3 * n_classes]\n# oof_stack = np.hstack([oof_lgb, oof_xgb, oof_cb])\n# test_stack = np.hstack([test_lgb, test_xgb, test_cb])\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 4️⃣ Cluster‑Based Weight Optimization\n# K = 8  # number of clusters\n# km = KMeans(n_clusters=K, random_state=42, n_init=10)\n# clusters = km.fit_predict(X_scaled)\n# test_clusters = km.predict(X_test_scaled)\n\n# # Prepare container for cluster‑specific weights\n# cluster_weights = np.zeros((K, 3))\n\n# # For each cluster, solve non‑negative least squares to minimize log-loss\n# for c in range(K):\n#     idx = np.where(clusters == c)[0]\n#     if len(idx) < n_classes:\n#         # too few samples: fallback to uniform weights\n#         cluster_weights[c] = [1/3, 1/3, 1/3]\n#         continue\n\n#     # build matrix A and vector b for nnls: \n#     # we’ll fit weights to approximate the one‑hot true labels\n#     A = np.vstack([\n#         oof_lgb[idx].flatten(),\n#         oof_xgb[idx].flatten(),\n#         oof_cb[idx].flatten()\n#     ]).T.reshape(-1, 3)\n#     # true occupancy vector b: for each sample and each class, 1 if true class, else 0\n#     true = np.zeros((len(idx)*n_classes,))\n#     for i, samp in enumerate(idx):\n#         true[i*n_classes + train['target'].iloc[samp]] = 1.0\n\n#     # solve nnls to get non‑negative weights w minimizing ||A w – true||²\n#     w, _ = nnls(A, true)\n#     cluster_weights[c] = w / w.sum()\n\n# print(\"Cluster Weights:\\n\", cluster_weights)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 5️⃣ Apply Cluster‑Specific Blending to Test Predictions\n# final_test = np.zeros_like(test_lgb)\n# for i in range(len(test)):\n#     c = test_clusters[i]\n#     w = cluster_weights[c]\n#     p_l = test_lgb[i]\n#     p_x = test_xgb[i]\n#     p_c = test_cb[i]\n#     final_test[i] = w[0]*p_l + w[1]*p_x + w[2]*p_c\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 6️⃣ Build Final Submission\n# top3 = np.argsort(final_test, axis=1)[:, -3:][:, ::-1]\n# labels = le.inverse_transform(top3.ravel()).reshape(top3.shape)\n\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in labels]\n# })\n\n# submission.to_csv('submission_dynamic_cluster_blend.csv', index=False)\n# print(\"✅ submission_dynamic_cluster_blend.csv saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T14:13:15.619533Z","iopub.execute_input":"2025-06-30T14:13:15.620708Z","iopub.status.idle":"2025-06-30T14:47:18.882240Z","shell.execute_reply.started":"2025-06-30T14:13:15.620671Z","shell.execute_reply":"2025-06-30T14:47:18.881119Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# One-hot encoding,Logistic regression model, Augmentation using the original dataset (","metadata":{}},{"cell_type":"code","source":"# 1️⃣ Imports\nimport pandas as pd\nimport numpy as np\nfrom itertools import combinations\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mutual_info_score, make_scorer\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.base import BaseEstimator, ClassifierMixin, clone\nfrom sklearn.utils.parallel import Parallel, delayed\nfrom tqdm import tqdm\n\n# 2️⃣ Load train + external original dataset\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv', index_col='id')\ntest  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv', index_col='id')\nextra = pd.read_csv('/kaggle/input/fertilizer-prediction-df/Fertilizer Prediction.csv')\nextra.columns = extra.columns.str.strip()\n\ntrain.rename(columns={'Temparature': 'Temperature'}, inplace=True)\ntest.rename(columns={'Temparature': 'Temperature'}, inplace=True)\nextra.rename(columns={'Temparature': 'Temperature','Humidity ': 'Humidity'}, inplace=True)\n\n# Encode target\nle = LabelEncoder()\ny = le.fit_transform(train.pop('Fertilizer Name'))\ny_extra = le.transform(extra.pop('Fertilizer Name'))\n\n# Treat all features as categorical\ntrain = train.astype(str)\ntest = test.astype(str)\nextra = extra.astype(str)\nX_all = pd.concat([train, extra], axis=0).reset_index(drop=True)\n\n# 3️⃣ Mutual Information based selection of 3-way combos\ndef adjusted_mutual_info(x, y, n_iter=5):\n    x, y = x.astype(str), y.astype(str)\n    m0 = mutual_info_score(x, y)\n    m1 = Parallel(n_jobs=-1)(delayed(lambda rs: mutual_info_score(\n        y, np.random.default_rng(rs).permutation(x)))(rs) for rs in range(n_iter))\n    return m0 - np.mean(m1)\n\n# Compute 3-way combos\ne = mutual_info_score(y, y)\nmi = {}\nfor c1, c2, c3 in tqdm(list(combinations(train.columns, 3))):\n    col_name = f'{c1}_{c2}_{c3}'\n    combined = X_all[c1] + '_' + X_all[c2] + '_' + X_all[c3]\n    mi[col_name] = adjusted_mutual_info(combined, np.concatenate([y, y_extra])) / e\n\ntop_3_combos = sorted(mi, key=mi.get, reverse=True)[:10]  # Select top 10 combos\n\n# 4️⃣ Create 2-combo + selected 3-combo features\ndef create_cross_features(df):\n    df_new = df.copy()\n    for c1, c2 in combinations(df.columns, 2):\n        df_new[f'{c1}_{c2}'] = df[c1] + '_' + df[c2]\n    for col in top_3_combos:\n        c1, c2, c3 = col.split('_')\n        df_new[col] = df[c1] + '_' + df[c2] + '_' + df[c3]\n    return df_new\n\nX_train_full = create_cross_features(train)\nX_extra_full = create_cross_features(extra)\nX_test_full  = create_cross_features(test)\n\n# 5️⃣ Augmented Logistic Regression class\ndef Augmented(model, X_o, y_o, weight_arg='sample_weight', weight=1.0):\n    class AugmentedModel(ClassifierMixin, BaseEstimator):\n        def fit(self, X, y):\n            sample_weight = np.array([1.0]*len(X)+[weight]*len(X_o))\n            X_combined = pd.concat([X, X_o])\n            y_combined = np.concatenate([y, y_o])\n            self.m = clone(model).fit(X_combined, y_combined, **{weight_arg: sample_weight})\n            self.classes_ = self.m.classes_\n            return self\n        def predict_proba(self, X):\n            return self.m.predict_proba(X)\n    return AugmentedModel()\n\n# 6️⃣ MAP@3 scorer\ndef MAP(k):\n    def mapk(y_true, y_pred):        \n        y_pred = np.argsort(-y_pred, axis=1)[:, :k]\n        m = (y_true[:, None] == y_pred)\n        return np.mean(np.where(m.any(axis=1), 1/(m.argmax(axis=1)+1), 0))\n    return mapk\n\nMAP3 = MAP(3)\nscorer = make_scorer(MAP3, response_method='predict_proba')\n\n# 7️⃣ Train final model\nmodel = Augmented(\n    make_pipeline(\n        OneHotEncoder(handle_unknown='ignore'),\n        LogisticRegression(C=1e-2, max_iter=1000, random_state=0)\n    ),\n    X_extra_full, y_extra,\n    weight_arg='logisticregression__sample_weight', weight=4.0\n)\n\nprint(\"🧠 Training final model...\")\nmodel.fit(X_train_full, y)\n\n# 8️⃣ Predict and prepare submission\nprobs = model.predict_proba(X_test_full)\ntop3 = np.argsort(-probs, axis=1)[:, :3]\nlabels = le.inverse_transform(top3.ravel()).reshape(top3.shape)\n\nsubmission = pd.DataFrame({\n    'id': test.index,\n    'Fertilizer Name': [' '.join(row) for row in labels]\n})\nsubmission.to_csv(\"submission_logistic_ensemble.csv\", index=False)\nprint(\"✅ Saved: submission_logistic_ensemble.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T05:56:06.372587Z","iopub.execute_input":"2025-07-01T05:56:06.372919Z","iopub.status.idle":"2025-07-01T06:18:05.631692Z","shell.execute_reply.started":"2025-07-01T05:56:06.372896Z","shell.execute_reply":"2025-07-01T06:18:05.630192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_full.shape[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T05:23:21.440642Z","iopub.execute_input":"2025-07-01T05:23:21.441004Z","iopub.status.idle":"2025-07-01T05:23:21.447909Z","shell.execute_reply.started":"2025-07-01T05:23:21.440969Z","shell.execute_reply":"2025-07-01T05:23:21.446768Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CatBoost with cross-feature + bagging ensemble ","metadata":{}},{"cell_type":"code","source":"# # ✅ CatBoost with feature interactions (CPU-compatible)\n\n# import pandas as pd\n# import numpy as np\n# from catboost import CatBoostClassifier, Pool\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.preprocessing import LabelEncoder\n\n# # Load data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix column name typo\n# train.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n\n# # Encode target\n# le = LabelEncoder()\n# train['target'] = le.fit_transform(train['Fertilizer Name'])\n# n_classes = train['target'].nunique()\n\n# # Create cross feature\n# train['Soil_Crop'] = train['Soil Type'] + \"_\" + train['Crop Type']\n# test['Soil_Crop'] = test['Soil Type'] + \"_\" + test['Crop Type']\n\n# # Categorical features\n# cat_features = ['Soil Type', 'Crop Type', 'Soil_Crop']\n\n# # Feature list\n# features = ['Temperature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous'] + cat_features\n# X = train[features]\n# y = train['target']\n# X_test = test[features]\n\n# # Stratified CV with CatBoost\n# test_preds = np.zeros((X_test.shape[0], n_classes))\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n#     print(f\"Training fold {fold + 1}/5\")\n    \n#     X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n#     X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n\n#     train_pool = Pool(X_train, y_train, cat_features=cat_features)\n#     val_pool = Pool(X_val, y_val, cat_features=cat_features)\n#     test_pool = Pool(X_test, cat_features=cat_features)\n\n#     model = CatBoostClassifier(\n#         iterations=2000,\n#         learning_rate=0.05,\n#         depth=7,\n#         eval_metric='MultiClass',\n#         random_seed=42,\n#         verbose=0,\n#         early_stopping_rounds=50,\n#         task_type='CPU'  # ✅ Use CPU to avoid CUDA error\n#     )\n\n#     model.fit(train_pool, eval_set=val_pool, use_best_model=True)\n#     test_preds += model.predict_proba(test_pool) / skf.n_splits\n\n# # Submission\n# top3 = np.argsort(test_preds, axis=1)[:, -3:][:, ::-1]\n# labels = le.inverse_transform(top3.ravel()).reshape(top3.shape)\n\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in labels]\n# })\n\n# submission.to_csv('catboost_feature_interaction_cpu.csv', index=False)\n# print(\"✅ Submission saved: catboost_feature_interaction_cpu.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T12:14:04.217152Z","iopub.execute_input":"2025-06-29T12:14:04.217621Z","iopub.status.idle":"2025-06-29T12:48:08.055620Z","shell.execute_reply.started":"2025-06-29T12:14:04.217592Z","shell.execute_reply":"2025-06-29T12:48:08.053855Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LightGBM + KMeans Cluster Features + Optuna + Bagging","metadata":{}},{"cell_type":"code","source":"# # 📦 Imports\n# import pandas as pd\n# import numpy as np\n# import lightgbm as lgb\n# import optuna\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.metrics import log_loss\n# from sklearn.cluster import KMeans\n# from sklearn.preprocessing import StandardScaler\n\n# # 🧠 Load data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix typo\n# train.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n\n# # Encode target\n# le = LabelEncoder()\n# train['target'] = le.fit_transform(train['Fertilizer Name'])\n# n_classes = train['target'].nunique()\n\n# # Target-mean encoding\n# for col in ['Soil Type', 'Crop Type']:\n#     m = train.groupby(col)['target'].mean()\n#     train[f'{col}_enc'] = train[col].map(m)\n#     test[f'{col}_enc'] = test[col].map(m)\n\n# # Base features\n# features = ['Temperature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous',\n#             'Soil Type_enc', 'Crop Type_enc']\n\n# # Scale numeric features for clustering\n# scaler = StandardScaler()\n# X_for_cluster = scaler.fit_transform(train[features])\n\n# # 🌀 Add KMeans cluster labels\n# kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n# train['cluster'] = kmeans.fit_predict(X_for_cluster)\n# test['cluster'] = kmeans.predict(scaler.transform(test[features]))\n\n# # Add cluster to features\n# features.append('cluster')\n\n# X = train[features].values\n# y = train['target'].values\n# X_test = test[features].values\n\n# # 🔍 Optuna tuning objective\n# def objective(trial):\n#     param = {\n#         'objective': 'multiclass',\n#         'num_class': n_classes,\n#         'metric': 'multi_logloss',\n#         'verbosity': -1,\n#         'boosting_type': 'gbdt',\n#         'learning_rate': trial.suggest_float('lr', 0.01, 0.3, log=True),\n#         'num_leaves': trial.suggest_int('num_leaves', 16, 256),\n#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 200),\n#         'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n#         'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 10.0),\n#         'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 10.0),\n#     }\n#     skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n#     losses = []\n#     for train_idx, val_idx in skf.split(X, y):\n#         dtrain = lgb.Dataset(X[train_idx], label=y[train_idx])\n#         dval = lgb.Dataset(X[val_idx], label=y[val_idx])\n#         gbm = lgb.train(\n#             param, dtrain,\n#             num_boost_round=1000,\n#             valid_sets=[dval],\n#             callbacks=[\n#                 lgb.early_stopping(stopping_rounds=50),\n#                 lgb.log_evaluation(period=0)\n#             ],\n#         )\n#         preds = gbm.predict(X[val_idx])\n#         losses.append(log_loss(y[val_idx], preds))\n#     return np.mean(losses)\n\n# # 🔧 Run Optuna\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=40, timeout=600)\n# best_params = study.best_params\n# print(\"Best params:\", best_params)\n\n# # 📦 Bagging + Final Predictions\n# BAGS = 5\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# test_preds = np.zeros((X_test.shape[0], n_classes))\n\n# for bag in range(BAGS):\n#     print(f\"Bag {bag + 1}/{BAGS}\")\n#     seed = 1000 + bag\n#     preds_oof = np.zeros_like(test_preds)\n\n#     for train_idx, val_idx in skf.split(X, y):\n#         dtrain = lgb.Dataset(X[train_idx], label=y[train_idx])\n#         dval = lgb.Dataset(X[val_idx], label=y[val_idx])\n\n#         gbm = lgb.train(\n#             {**best_params, 'seed': seed, 'objective': 'multiclass', 'num_class': n_classes, 'metric': 'multi_logloss'},\n#             dtrain,\n#             num_boost_round=1000,\n#             valid_sets=[dval],\n#             callbacks=[lgb.early_stopping(50)],\n#         )\n\n#         preds_oof += gbm.predict(X_test) / skf.n_splits\n\n#     test_preds += preds_oof / BAGS\n\n# # 📝 Submission\n# top3 = np.argsort(test_preds, axis=1)[:, -3:][:, ::-1]\n# labels = le.inverse_transform(top3.ravel()).reshape(top3.shape)\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in labels]\n# })\n# submission.to_csv('submission_lgbm_clustered.csv', index=False)\n# print(\"✅ submission_lgbm_clustered.csv saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stacking_LGB_XGB_CatB_NNMeta_v1","metadata":{}},{"cell_type":"code","source":"# # ───────────────────────────────────────────────────────────────────────────────\n# # 📦 Install dependencies (run once)\n# !pip install lightgbm xgboost catboost tensorflow --quiet\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 1️⃣ Imports\n# import pandas as pd\n# import numpy as np\n# from sklearn.preprocessing import LabelEncoder, StandardScaler\n# from sklearn.model_selection import StratifiedKFold\n# import lightgbm as lgb\n# import xgboost as xgb\n# import catboost as cb\n# import tensorflow as tf\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense, Dropout\n# from tensorflow.keras.callbacks import EarlyStopping\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 2️⃣ Load & Preprocess Data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix typo\n# train.rename(columns={'Temparature':'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature':'Temperature'}, inplace=True)\n\n# # Encode target\n# le = LabelEncoder()\n# train['target'] = le.fit_transform(train['Fertilizer Name'])\n# n_classes = train['target'].nunique()\n\n# # Basic features (no heavy engineering for speed)\n# features = ['Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous']\n# X = train[features].values\n# y = train['target'].values\n# X_test = test[features].values\n\n# # Scale features for meta‑model stability\n# scaler = StandardScaler()\n# X = scaler.fit_transform(X)\n# X_test = scaler.transform(X_test)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 3️⃣ Generate Base‑Model OOF & Test Predictions\n# n_splits = 5\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# # Storage arrays\n# oof_preds = np.zeros((X.shape[0], n_classes * 3))\n# test_preds = np.zeros((X_test.shape[0], n_classes * 3))\n\n# for fold, (tr, val) in enumerate(skf.split(X, y), 1):\n#     print(f\"▶ Fold {fold}/{n_splits}\")\n#     X_tr, X_val = X[tr], X[val]\n#     y_tr, y_val = y[tr], y[val]\n\n#     # — LightGBM\n#     m1 = lgb.LGBMClassifier(\n#         objective='multiclass', num_class=n_classes,\n#         learning_rate=0.1, n_estimators=200, random_state=fold\n#     )\n#     m1.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50)])\n#     oof_preds[val,   0*n_classes:1*n_classes] = m1.predict_proba(X_val)\n#     test_preds[:,   0*n_classes:1*n_classes] += m1.predict_proba(X_test) / n_splits\n\n#     # — XGBoost\n#     m2 = xgb.XGBClassifier(\n#         objective='multi:softprob', num_class=n_classes,\n#         learning_rate=0.1, n_estimators=200,\n#         use_label_encoder=False, eval_metric='mlogloss',\n#         random_state=fold\n#     )\n#     m2.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)\n#     oof_preds[val,   1*n_classes:2*n_classes] = m2.predict_proba(X_val)\n#     test_preds[:,   1*n_classes:2*n_classes] += m2.predict_proba(X_test) / n_splits\n\n#     # — CatBoost\n#     m3 = cb.CatBoostClassifier(\n#         iterations=200, learning_rate=0.1, depth=6,\n#         loss_function='MultiClass', verbose=0, random_seed=fold\n#     )\n#     m3.fit(X_tr, y_tr)\n#     oof_preds[val,   2*n_classes:3*n_classes] = m3.predict_proba(X_val)\n#     test_preds[:,   2*n_classes:3*n_classes] += m3.predict_proba(X_test) / n_splits\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 4️⃣ Build & Train Meta‑Learner (Keras NN)\n# # One‑hot encode the OOF targets\n# y_ohe = tf.keras.utils.to_categorical(y, num_classes=n_classes)\n\n# # Simple MLP\n# meta = Sequential([\n#     Dense(64, activation='relu', input_shape=(oof_preds.shape[1],)),\n#     Dropout(0.3),\n#     Dense(32, activation='relu'),\n#     Dropout(0.2),\n#     Dense(n_classes, activation='softmax')\n# ])\n# meta.compile(optimizer='adam', loss='categorical_crossentropy')\n\n# # Early stopping on OOF loss\n# es = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n\n# # Train on full OOF set\n# meta.fit(oof_preds, y_ohe, epochs=50, batch_size=1024, callbacks=[es], verbose=2)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 5️⃣ Final Predictions & Submission\n# meta_probs = meta.predict(test_preds, batch_size=1024)\n# top3 = np.argsort(meta_probs, axis=1)[:, -3:][:, ::-1]\n# labels = le.inverse_transform(top3.ravel()).reshape(top3.shape)\n\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in labels]\n# })\n# submission.to_csv('submission_stacked_nn_fast.csv', index=False)\n# print(\"✅ submission_stacked_nn_fast.csv saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LightGBM, XGBoost, CatBoost (5-fold CV)","metadata":{}},{"cell_type":"code","source":"# # ───────────────────────────────────────────────────────────────────────────────\n# # 📦 Install dependencies (run once)\n# !pip install lightgbm xgboost catboost --quiet\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 1️⃣ Imports\n# import pandas as pd\n# import numpy as np\n# from sklearn.preprocessing import LabelEncoder, StandardScaler\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.linear_model import LogisticRegression\n# import lightgbm as lgb\n# import xgboost as xgb\n# import catboost as cb\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 2️⃣ Load & Preprocess Data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix column typo\n# train.rename(columns={'Temparature':'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature':'Temperature'}, inplace=True)\n\n# # Encode target\n# le = LabelEncoder()\n# train['target'] = le.fit_transform(train['Fertilizer Name'])\n# n_classes = train['target'].nunique()\n\n# # Target-mean encoding for categoricals\n# for col in ['Soil Type','Crop Type']:\n#     m = train.groupby(col)['target'].mean()\n#     train[f'{col}_te'] = train[col].map(m)\n#     test[f'{col}_te']  = test[col].map(m)\n\n# # Feature list\n# features = [\n#     'Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous',\n#     'Soil Type_te','Crop Type_te'\n# ]\n\n# X = train[features].values\n# y = train['target'].values\n# X_test = test[features].values\n\n# # Scale features\n# scaler = StandardScaler()\n# X = scaler.fit_transform(X)\n# X_test = scaler.transform(X_test)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 3️⃣ Prepare Out-of-Fold Predictions\n# n_splits = 5\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# # We will store OOF and test predictions for each model\n# oof_preds = np.zeros((X.shape[0], n_classes * 3))\n# test_preds = np.zeros((X_test.shape[0], n_classes * 3))\n\n# for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), 1):\n#     print(f\"▶ Fold {fold}/{n_splits}\")\n\n#     X_tr, X_val = X[tr_idx], X[val_idx]\n#     y_tr, y_val = y[tr_idx], y[val_idx]\n\n#     # LightGBM\n#     m_lgb = lgb.LGBMClassifier(\n#         objective='multiclass', num_class=n_classes,\n#         learning_rate=0.05, n_estimators=1000, random_state=fold\n#     )\n#     m_lgb.fit(\n#         X_tr, y_tr,\n#         eval_set=[(X_val, y_val)],\n#         callbacks=[\n#             lgb.early_stopping(stopping_rounds=50),\n#             lgb.log_evaluation(period=0)\n#         ]\n#     )\n#     oof_preds[val_idx, 0*n_classes:1*n_classes] = m_lgb.predict_proba(X_val)\n#     test_preds[:,   0*n_classes:1*n_classes] += m_lgb.predict_proba(X_test) / n_splits\n\n#     # XGBoost\n#     m_xgb = xgb.XGBClassifier(\n#         objective='multi:softprob', num_class=n_classes,\n#         learning_rate=0.05, n_estimators=1000,\n#         use_label_encoder=False, eval_metric='mlogloss',\n#         random_state=fold\n#     )\n#     m_xgb.fit(\n#         X_tr, y_tr,\n#         eval_set=[(X_val, y_val)],\n#         early_stopping_rounds=50,\n#         verbose=False\n#     )\n#     oof_preds[val_idx, 1*n_classes:2*n_classes] = m_xgb.predict_proba(X_val)\n#     test_preds[:,   1*n_classes:2*n_classes] += m_xgb.predict_proba(X_test) / n_splits\n\n#     # CatBoost\n#     m_cat = cb.CatBoostClassifier(\n#         iterations=1000, learning_rate=0.05, depth=6,\n#         loss_function='MultiClass', eval_metric='MultiClass',\n#         verbose=0, random_seed=fold\n#     )\n#     m_cat.fit(X_tr, y_tr)\n#     oof_preds[val_idx, 2*n_classes:3*n_classes] = m_cat.predict_proba(X_val)\n#     test_preds[:,   2*n_classes:3*n_classes] += m_cat.predict_proba(X_test) / n_splits\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 4️⃣ Train Meta-Model (Logistic Regression) on full OOF predictions\n# meta = LogisticRegression(\n#     multi_class='multinomial',\n#     solver='lbfgs',\n#     C=1.0,\n#     max_iter=1000,\n#     random_state=42\n# )\n# meta.fit(oof_preds, y)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 5️⃣ Final Predict & Submission\n# meta_probs = meta.predict_proba(test_preds)\n\n# # Extract top-3 labels per sample\n# top3 = np.argsort(meta_probs, axis=1)[:, -3:][:, ::-1]\n# labels = le.inverse_transform(top3.ravel()).reshape(top3.shape)\n\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in labels]\n# })\n# submission.to_csv('submission_stacked_lr.csv', index=False)\n# print(\"✅ submission_stacked_lr.csv saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stacking Ensemble (LGBM + CatBoost + XGBoost → Neural Net)","metadata":{}},{"cell_type":"code","source":"# # ───────────────────────────────────────────────────────────────────────────────\n# # 📦 Install dependencies (run once)\n# !pip install lightgbm xgboost catboost tensorflow --quiet\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 1️⃣ Imports\n# import pandas as pd\n# import numpy as np\n# from sklearn.preprocessing import LabelEncoder, StandardScaler\n# from sklearn.model_selection import StratifiedKFold\n# import lightgbm as lgb\n# import xgboost as xgb\n# import catboost as cb\n# import tensorflow as tf\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense, Dropout\n# from tensorflow.keras.callbacks import EarlyStopping\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 2️⃣ Load & Preprocess Data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix typo\n# train.rename(columns={'Temparature':'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature':'Temperature'}, inplace=True)\n\n# # Encode target\n# le = LabelEncoder()\n# train['target'] = le.fit_transform(train['Fertilizer Name'])\n# n_classes = train['target'].nunique()\n\n# # Basic features (no heavy engineering for speed)\n# features = ['Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous']\n# X = train[features].values\n# y = train['target'].values\n# X_test = test[features].values\n\n# # Scale features for meta‑model stability\n# scaler = StandardScaler()\n# X = scaler.fit_transform(X)\n# X_test = scaler.transform(X_test)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 3️⃣ Generate Base‑Model OOF & Test Predictions\n# n_splits = 5\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# # Storage arrays\n# oof_preds = np.zeros((X.shape[0], n_classes * 3))\n# test_preds = np.zeros((X_test.shape[0], n_classes * 3))\n\n# for fold, (tr, val) in enumerate(skf.split(X, y), 1):\n#     print(f\"▶ Fold {fold}/{n_splits}\")\n#     X_tr, X_val = X[tr], X[val]\n#     y_tr, y_val = y[tr], y[val]\n\n#     # — LightGBM\n#     m1 = lgb.LGBMClassifier(\n#         objective='multiclass', num_class=n_classes,\n#         learning_rate=0.1, n_estimators=200, random_state=fold\n#     )\n#     m1.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50)])\n#     oof_preds[val,   0*n_classes:1*n_classes] = m1.predict_proba(X_val)\n#     test_preds[:,   0*n_classes:1*n_classes] += m1.predict_proba(X_test) / n_splits\n\n#     # — XGBoost\n#     m2 = xgb.XGBClassifier(\n#         objective='multi:softprob', num_class=n_classes,\n#         learning_rate=0.1, n_estimators=200,\n#         use_label_encoder=False, eval_metric='mlogloss',\n#         random_state=fold\n#     )\n#     m2.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)\n#     oof_preds[val,   1*n_classes:2*n_classes] = m2.predict_proba(X_val)\n#     test_preds[:,   1*n_classes:2*n_classes] += m2.predict_proba(X_test) / n_splits\n\n#     # — CatBoost\n#     m3 = cb.CatBoostClassifier(\n#         iterations=200, learning_rate=0.1, depth=6,\n#         loss_function='MultiClass', verbose=0, random_seed=fold\n#     )\n#     m3.fit(X_tr, y_tr)\n#     oof_preds[val,   2*n_classes:3*n_classes] = m3.predict_proba(X_val)\n#     test_preds[:,   2*n_classes:3*n_classes] += m3.predict_proba(X_test) / n_splits\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 4️⃣ Build & Train Meta‑Learner (Keras NN)\n# # One‑hot encode the OOF targets\n# y_ohe = tf.keras.utils.to_categorical(y, num_classes=n_classes)\n\n# # Simple MLP\n# meta = Sequential([\n#     Dense(64, activation='relu', input_shape=(oof_preds.shape[1],)),\n#     Dropout(0.3),\n#     Dense(32, activation='relu'),\n#     Dropout(0.2),\n#     Dense(n_classes, activation='softmax')\n# ])\n# meta.compile(optimizer='adam', loss='categorical_crossentropy')\n\n# # Early stopping on OOF loss\n# es = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n\n# # Train on full OOF set\n# meta.fit(oof_preds, y_ohe, epochs=100, batch_size=1024, callbacks=[es], verbose=2)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 5️⃣ Final Predictions & Submission\n# meta_probs = meta.predict(test_preds, batch_size=1024)\n# top3 = np.argsort(meta_probs, axis=1)[:, -3:][:, ::-1]\n# labels = le.inverse_transform(top3.ravel()).reshape(top3.shape)\n\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in labels]\n# })\n# submission.to_csv('submission_stacked_nn_fast.csv', index=False)\n# print(\"✅ submission_stacked_nn_fast.csv saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# QuantileTransform + Smart Cross-Features ","metadata":{}},{"cell_type":"code","source":"# # ───────────────────────────────────────────────────────────────────────────────\n# # 📦 Install dependencies (run once)\n# !pip install lightgbm --quiet\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 1️⃣ Imports\n# import pandas as pd\n# import numpy as np\n# import lightgbm as lgb\n# from sklearn.preprocessing import LabelEncoder, QuantileTransformer\n# from sklearn.model_selection import StratifiedKFold\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 2️⃣ Load & Preprocess Data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix typo\n# train.rename(columns={'Temparature':'Temperature'}, inplace=True)\n# test .rename(columns={'Temparature':'Temperature'}, inplace=True)\n\n# # Encode target\n# le = LabelEncoder()\n# train['target'] = le.fit_transform(train['Fertilizer Name'])\n# n_classes = train['target'].nunique()\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 3️⃣ Smart Cross-Features\n\n# # Ratio features\n# train['N_P_ratio'] = train['Nitrogen'] / (train['Phosphorous'] + 1)\n# test ['N_P_ratio'] = test ['Nitrogen'] / (test ['Phosphorous'] + 1)\n\n# train['T_minus_H_over_M'] = (train['Temperature'] - train['Humidity']) / (train['Moisture'] + 1)\n# test ['T_minus_H_over_M'] = (test ['Temperature'] - test ['Humidity']) / (test ['Moisture'] + 1)\n\n# # Soil×Crop one-hot\n# train['Soil_Crop'] = train['Soil Type'] + '_' + train['Crop Type']\n# test ['Soil_Crop'] = test ['Soil Type'] + '_' + test ['Crop Type']\n# dummies = pd.get_dummies(pd.concat([train['Soil_Crop'], test['Soil_Crop']]), prefix='SC')\n# train_sc = dummies.iloc[:len(train)]\n# test_sc  = dummies.iloc[len(train):].reset_index(drop=True)\n\n# # Target‐mean encoding (for comparison)\n# for col in ['Soil Type','Crop Type']:\n#     m = train.groupby(col)['target'].mean()\n#     train[f'{col}_te'] = train[col].map(m)\n#     test [f'{col}_te'] = test [col].map(m)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 4️⃣ Assemble Feature Matrix\n\n# # Numeric & ratio columns\n# numeric_cols = [\n#     'Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous',\n#     'N_P_ratio','T_minus_H_over_M'\n# ]\n# # TE columns\n# te_cols = ['Soil Type_te','Crop Type_te']\n\n# # Combine\n# X_num = train[numeric_cols + te_cols + ['target']].drop(columns='target')\n# X_ratios = train[['N_P_ratio','T_minus_H_over_M']]\n# X_num.reset_index(drop=True, inplace=True)\n# X_train_full = pd.concat([X_num, train_sc.reset_index(drop=True)], axis=1)\n\n# X_test_num = test[numeric_cols + te_cols]\n# X_test_full = pd.concat([X_test_num.reset_index(drop=True), test_sc], axis=1)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 5️⃣ Quantile Transform on numeric + ratio\n# qt = QuantileTransformer(output_distribution='normal')\n# qt_cols = numeric_cols + ['Soil Type_te','Crop Type_te','N_P_ratio','T_minus_H_over_M']\n# X_train_full[qt_cols] = qt.fit_transform(X_train_full[qt_cols])\n# X_test_full [qt_cols] = qt.transform(X_test_full [qt_cols])\n\n# # Final feature matrices\n# X = X_train_full.values\n# X_test = X_test_full.values\n# y = train['target'].values\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 6️⃣ LightGBM Params (from Optuna best)\n# params = {\n#     'objective': 'multiclass',\n#     'num_class': n_classes,\n#     'metric': 'multi_logloss',\n#     'boosting_type': 'gbdt',\n#     'learning_rate': 0.0326699070625641,\n#     'num_leaves': 225,\n#     'min_data_in_leaf': 112,\n#     'feature_fraction': 0.5899214712446607,\n#     'bagging_fraction': 0.7276943630263858,\n#     'bagging_freq': 2,\n#     'lambda_l1': 2.8479755333998225,\n#     'lambda_l2': 4.199355747289704,\n#     'verbosity': -1\n# }\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 7️⃣ 5-Fold CV + Bagging Predictions\n# BAGS = 5\n# skf = StratifiedKFold(n_splits=BAGS, shuffle=True, random_state=42)\n# test_preds = np.zeros((len(test), n_classes))\n\n# for bag in range(BAGS):\n#     print(f\"Bag {bag+1}/{BAGS}\")\n#     seed = 2025 + bag\n#     fold_preds = np.zeros_like(test_preds)\n\n#     for train_idx, val_idx in skf.split(X, y):\n#         X_tr, X_val = X[train_idx], X[val_idx]\n#         y_tr, y_val = y[train_idx], y[val_idx]\n\n#         dtrain = lgb.Dataset(X_tr, label=y_tr)\n#         dval   = lgb.Dataset(X_val, label=y_val)\n#         gbm = lgb.train(\n#             {**params, 'seed': seed},\n#             dtrain,\n#             num_boost_round=1000,\n#             valid_sets=[dval],\n#             callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=0)]\n#         )\n#         fold_preds += gbm.predict(X_test) / skf.n_splits\n\n#     test_preds += fold_preds / BAGS\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 8️⃣ Make Submission\n# top3 = np.argsort(test_preds, axis=1)[:, -3:][:, ::-1]\n# labels = le.inverse_transform(top3.ravel()).reshape(top3.shape)\n\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in labels]\n# })\n# submission.to_csv('submission_lgbm_quantile_cross.csv', index=False)\n# print(\"✅ submission_lgbm_quantile_cross.csv saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CatBoost + Bagging — strong baseline)","metadata":{}},{"cell_type":"code","source":"# # 1️⃣ Imports\n# import pandas as pd\n# import numpy as np\n# from catboost import CatBoostClassifier, Pool\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.preprocessing import LabelEncoder\n\n# # 2️⃣ Load & preprocess\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n# train.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n\n# # Target encoding\n# le = LabelEncoder()\n# train['target'] = le.fit_transform(train['Fertilizer Name'])\n# n_classes = train['target'].nunique()\n\n# # Features\n# features = ['Temperature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous',\n#             'Soil Type', 'Crop Type']\n# cat_features = ['Soil Type', 'Crop Type']\n# X = train[features]\n# y = train['target']\n# X_test = test[features]\n\n# # 3️⃣ Bagging setup\n# BAGS = 5\n# skf = StratifiedKFold(n_splits=BAGS, shuffle=True, random_state=42)\n# test_preds = np.zeros((len(test), n_classes))\n\n# # 4️⃣ Train CatBoost\n# for bag, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n#     print(f\"Bag {bag}/{BAGS}\")\n#     X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n#     y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n#     model = CatBoostClassifier(\n#         iterations=1000,\n#         learning_rate=0.05,\n#         depth=6,\n#         loss_function='MultiClass',\n#         eval_metric='MultiClass',\n#         verbose=0,\n#         random_seed=42 + bag,\n#         early_stopping_rounds=50\n#     )\n\n#     model.fit(\n#         Pool(X_train, y_train, cat_features=cat_features),\n#         eval_set=Pool(X_val, y_val, cat_features=cat_features)\n#     )\n#     preds = model.predict_proba(Pool(X_test, cat_features=cat_features))\n#     test_preds += preds / BAGS\n\n# # 5️⃣ Submission\n# top3 = np.argsort(test_preds, axis=1)[:, -3:][:, ::-1]\n# labels = le.inverse_transform(top3.ravel()).reshape(top3.shape)\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in labels]\n# })\n# submission.to_csv('submission_catboost_bag.csv', index=False)\n# print(\"✅ submission_catboost_bag.csv saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pytourch based","metadata":{}},{"cell_type":"code","source":"# # ───────────────────────────────────────────────────────────────────────────────\n# # 1️⃣ Install & Imports\n# !pip install torch --quiet\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import torch\n# import torch.nn as nn\n# from torch.utils.data import Dataset, DataLoader\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import train_test_split\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 2️⃣ Load & Preprocess Data\n# df = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n# df.rename(columns={'Temparature':'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature':'Temperature'}, inplace=True)\n\n# # Encode target\n# le_target = LabelEncoder()\n# df['target'] = le_target.fit_transform(df['Fertilizer Name'])\n# n_classes = df['target'].nunique()\n\n# # Prepare features\n# NUM_COLS = ['Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous']\n# CAT_COLS = ['Soil Type','Crop Type']\n\n# # Label-encode categoricals\n# encoders = {}\n# for col in CAT_COLS:\n#     le = LabelEncoder()\n#     df[col] = le.fit_transform(df[col])\n#     test[col] = le.transform(test[col])\n#     encoders[col] = le\n\n# # Dataset split\n# train_df, val_df = train_test_split(\n#     df, test_size=0.1, stratify=df['target'], random_state=42\n# )\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 3️⃣ Build PyTorch Dataset\n# class TabularDataset(Dataset):\n#     def __init__(self, df):\n#         self.X_num = df[NUM_COLS].values.astype(np.float32)\n#         self.X_cat = df[CAT_COLS].values.astype(np.int64)\n#         self.y     = df['target'].values.astype(np.int64)\n#     def __len__(self): return len(self.y)\n#     def __getitem__(self, i):\n#         return self.X_num[i], self.X_cat[i], self.y[i]\n\n# train_ds = TabularDataset(train_df)\n# val_ds   = TabularDataset(val_df)\n# test_num = test[NUM_COLS].values.astype(np.float32)\n# test_cat = test[CAT_COLS].values.astype(np.int64)\n\n# train_loader = DataLoader(train_ds, batch_size=4096, shuffle=True, drop_last=True)\n# val_loader   = DataLoader(val_ds,   batch_size=4096, shuffle=False)\n# test_loader  = DataLoader(list(zip(test_num, test_cat)), batch_size=4096, shuffle=False)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 4️⃣ Define Model with Embeddings + MLP\n# class EmbeddingMLP(nn.Module):\n#     def __init__(self, num_cols, cat_dims, emb_dims, hidden_dims, n_out):\n#         super().__init__()\n#         # embeddings\n#         self.embs = nn.ModuleList([\n#             nn.Embedding(cat_dim, emb_dim)\n#             for cat_dim, emb_dim in zip(cat_dims, emb_dims)\n#         ])\n#         input_dim = len(num_cols) + sum(emb_dims)\n#         # MLP layers\n#         layers = []\n#         prev = input_dim\n#         for h in hidden_dims:\n#             layers += [nn.Linear(prev, h), nn.ReLU(), nn.BatchNorm1d(h), nn.Dropout(0.2)]\n#             prev = h\n#         layers.append(nn.Linear(prev, n_out))\n#         self.net = nn.Sequential(*layers)\n\n#     def forward(self, x_num, x_cat):\n#         emb = [emb_layer(x_cat[:,i]) for i, emb_layer in enumerate(self.embs)]\n#         x = torch.cat([x_num] + emb, dim=1)\n#         return self.net(x)\n\n# # Embedding sizes: take sqrt of cardinality (common heuristic)\n# cat_dims = [df[col].nunique() for col in CAT_COLS]\n# emb_dims = [min(50, (cd+1)//2) for cd in cat_dims]\n# model = EmbeddingMLP(\n#     num_cols=NUM_COLS,\n#     cat_dims=cat_dims,\n#     emb_dims=emb_dims,\n#     hidden_dims=[256,128,64],\n#     n_out=n_classes\n# ).cuda()\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 5️⃣ Train Loop\n# criterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n# best_val_loss = np.inf\n# patience, trials = 3, 0\n\n# for epoch in range(20):\n#     # Train\n#     model.train()\n#     for Xn, Xc, yb in train_loader:\n#         Xn, Xc, yb = Xn.cuda(), Xc.cuda(), yb.cuda()\n#         preds = model(Xn, Xc)\n#         loss = criterion(preds, yb)\n#         optimizer.zero_grad(); loss.backward(); optimizer.step()\n#     # Validate\n#     model.eval()\n#     val_losses = []\n#     with torch.no_grad():\n#         for Xn, Xc, yb in val_loader:\n#             Xn, Xc, yb = Xn.cuda(), Xc.cuda(), yb.cuda()\n#             val_losses.append(criterion(model(Xn, Xc), yb).item())\n#     val_loss = np.mean(val_losses)\n#     print(f\"Epoch {epoch+1}: val_loss={val_loss:.4f}\")\n#     # Early stopping\n#     if val_loss < best_val_loss:\n#         best_val_loss = val_loss; torch.save(model.state_dict(), 'best_mlp.pt'); trials=0\n#     else:\n#         trials+=1\n#         if trials>=patience: break\n\n# # Load best\n# model.load_state_dict(torch.load('best_mlp.pt'))\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 6️⃣ Predict & Submission\n# model.eval(); all_probs = []\n# with torch.no_grad():\n#     for Xn, Xc in test_loader:\n#         Xn, Xc = Xn.cuda(), Xc.cuda()\n#         all_probs.append(torch.softmax(model(Xn, Xc), dim=1).cpu().numpy())\n# probs = np.vstack(all_probs)\n\n# top3 = np.argsort(probs, axis=1)[:, -3:][:, ::-1]\n# labels = le_target.inverse_transform(top3.ravel()).reshape(top3.shape)\n\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in labels]\n# })\n# submission.to_csv('submission_mlp_embed.csv', index=False)\n# print(\"✅ submission_mlp_embed.csv saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Wide & Deep in TensorFlow/Keras","metadata":{}},{"cell_type":"code","source":"# # ───────────────────────────────────────────────────────────────────────────────\n# # 1️⃣ Install & Imports\n# !pip install tensorflow --quiet\n\n# import pandas as pd\n# import numpy as np\n# import tensorflow as tf\n# from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.optimizers import Adam\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import train_test_split\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 2️⃣ Load & Preprocess\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n# train.rename(columns={'Temparature':'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature':'Temperature'}, inplace=True)\n\n# # Label-encode target\n# le_target = LabelEncoder()\n# train['target'] = le_target.fit_transform(train['Fertilizer Name'])\n# n_classes = train['target'].nunique()\n\n# # Numeric features\n# NUM_COLS = ['Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous']\n\n# # Categorical features\n# CAT_COLS = ['SoilType','CropType']\n# # First, rename columns to remove spaces\n# train.rename(columns={'Soil Type':'SoilType','Crop Type':'CropType'}, inplace=True)\n# test.rename(columns={'Soil Type':'SoilType','Crop Type':'CropType'}, inplace=True)\n\n# # Encode categories to integers\n# for col in CAT_COLS:\n#     enc = LabelEncoder()\n#     train[col] = enc.fit_transform(train[col])\n#     test[col]  = enc.transform(test[col])\n\n# # Split train/val for early stopping\n# X_train_df, X_val_df, y_train, y_val = train_test_split(\n#     train, train['target'], stratify=train['target'], test_size=0.1, random_state=42\n# )\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 3️⃣ Build Wide & Deep Model\n\n# # Wide input: SoilType × CropType interaction\n# interaction = Input(shape=(1,), name='soil_crop_inter', dtype='int32')\n\n# # Deep numeric input\n# num_input = Input(shape=(len(NUM_COLS),), name='num_input')\n\n# # Deep categorical inputs & embeddings\n# soil_in = Input(shape=(1,), name='SoilType_in', dtype='int32')\n# soil_emb = Embedding(input_dim=train['SoilType'].nunique(),\n#                      output_dim=min(50, train['SoilType'].nunique()//2),\n#                      name='SoilType_emb')(soil_in)\n# soil_emb_flat = Flatten()(soil_emb)\n\n# crop_in = Input(shape=(1,), name='CropType_in', dtype='int32')\n# crop_emb = Embedding(input_dim=train['CropType'].nunique(),\n#                      output_dim=min(50, train['CropType'].nunique()//2),\n#                      name='CropType_emb')(crop_in)\n# crop_emb_flat = Flatten()(crop_emb)\n\n# # Wide embedding layer acting as one-hot for interaction\n# max_interact = train['SoilType'].nunique() * train['CropType'].nunique()\n# wide_emb = Embedding(input_dim=max_interact,\n#                      output_dim=max_interact,\n#                      embeddings_initializer=tf.keras.initializers.Identity(),\n#                      trainable=False,\n#                      name='wide_emb')(interaction)\n# wide_flat = Flatten()(wide_emb)\n\n# # Deep tower\n# deep_concat = Concatenate(name='deep_concat')([\n#     num_input, soil_emb_flat, crop_emb_flat\n# ])\n# x = Dense(128, activation='relu')(deep_concat)\n# x = Dense(64, activation='relu')(x)\n# deep_out = Dense(32, activation='relu')(x)\n\n# # Combine wide + deep\n# combined = Concatenate(name='combined')([wide_flat, deep_out])\n# output = Dense(n_classes, activation='softmax', name='predictions')(combined)\n\n# model = Model(\n#     inputs=[interaction, num_input, soil_in, crop_in],\n#     outputs=output\n# )\n# model.compile(\n#     optimizer=Adam(learning_rate=1e-3),\n#     loss='sparse_categorical_crossentropy'\n# )\n\n# model.summary()\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 4️⃣ Prepare Inputs\n\n# def build_inputs(df):\n#     return {\n#         'soil_crop_inter': df['SoilType'] * train['CropType'].nunique() + df['CropType'],\n#         'num_input': df[NUM_COLS].values,\n#         'SoilType_in': df['SoilType'].values,\n#         'CropType_in': df['CropType'].values,\n#     }\n\n# train_inputs = build_inputs(X_train_df)\n# val_inputs   = build_inputs(X_val_df)\n# test_inputs  = build_inputs(test)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 5️⃣ Train\n\n# callbacks = [\n#     tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n# ]\n\n# model.fit(\n#     train_inputs, y_train.values,\n#     validation_data=(val_inputs, y_val.values),\n#     epochs=50, batch_size=2048,\n#     callbacks=callbacks,\n#     verbose=2\n# )\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 6️⃣ Predict & Create Submission\n\n# probs = model.predict(test_inputs, batch_size=2048)\n# top3 = np.argsort(probs, axis=1)[:, -3:][:, ::-1]\n# labels = le_target.inverse_transform(top3.ravel()).reshape(top3.shape)\n\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in labels]\n# })\n# submission.to_csv('submission_widedeep_corrected.csv', index=False)\n# print(\"✅ submission_widedeep_corrected.csv saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TabPFN","metadata":{}},{"cell_type":"code","source":"# pip install tabpfn\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ───────────────────────────────────────────────────────────────────────────────\n# # 1️⃣ Install & Imports\n# !pip install tabpfn --quiet\n\n# import pandas as pd\n# import numpy as np\n# import torch\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import train_test_split\n# from tabpfn import TabPFNClassifier\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 2️⃣ Load & Preprocess Data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix typo\n# train.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n\n# # Label‐encode target\n# le = LabelEncoder()\n# train['target'] = le.fit_transform(train['Fertilizer Name'])\n\n# # Define features (you can swap in your engineered features here)\n# features = ['Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous']\n# X_full = train[features]\n# y_full = train['target']\n# X_test = test[features]\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 3️⃣ Subsample for TabPFN (<=10 000 rows)\n# SAMPLE_SIZE = 10000\n# rng = np.random.RandomState(42)\n# idx = rng.choice(len(X_full), size=SAMPLE_SIZE, replace=False)\n# X_sub = X_full.iloc[idx].to_numpy()\n# y_sub = y_full.iloc[idx].to_numpy()\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 4️⃣ Train TabPFN\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# model = TabPFNClassifier(device=device)\n# model.fit(X_sub, y_sub)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 5️⃣ Predict & Build Submission\n# probs = model.predict_proba(X_test.to_numpy())  # shape (n_test, n_classes)\n# top3 = np.argsort(probs, axis=1)[:, -3:][:, ::-1]\n# labels = le.inverse_transform(top3.ravel()).reshape(top3.shape)\n\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in labels]\n# })\n# submission.to_csv('submission_tabpfn.csv', index=False)\n# print(\"✅ submission_tabpfn.csv saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LightGBM and CatBoost mix","metadata":{}},{"cell_type":"code","source":"# # ───────────────────────────────────────────────────────────────────────────────\n# # 📦 1. Install dependencies (run this cell once at the top)\n# !pip install lightgbm catboost --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ───────────────────────────────────────────────────────────────────────────────\n# # 🔧 2. Imports\n# import pandas as pd\n# import numpy as np\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import log_loss\n# import lightgbm as lgb\n# from catboost import CatBoostClassifier, Pool\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 📂 3. Load & Fix Data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n# train.rename(columns={'Temparature':'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature':'Temperature'}, inplace=True)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 🧹 4. Smoothing-Regularized Target Encoding\n# target = 'Fertilizer Name'\n# le_target = LabelEncoder()\n# train['target_enc'] = le_target.fit_transform(train[target])\n\n# global_mean = train['target_enc'].mean()\n# k_smooth = 30\n\n# for col in ['Soil Type','Crop Type']:\n#     stats = train.groupby(col)['target_enc'].agg(['mean','count'])\n#     smooth = (stats['count'] * stats['mean'] + k_smooth * global_mean) / (stats['count'] + k_smooth)\n#     train[f'{col}_te'] = train[col].map(smooth)\n#     test[f'{col}_te']  = test[col].map(smooth)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 💡 5. Cross-Feature Interactions\n# train['N_K_ratio']   = train['Nitrogen'] / (train['Potassium'] + 1)\n# test['N_K_ratio']    = test['Nitrogen']  / (test['Potassium'] + 1)\n\n# train['T_minus_H']   = train['Temperature'] - train['Humidity']\n# test['T_minus_H']    = test['Temperature'] - test['Humidity']\n\n# # label-encode Soil/Crop for interaction\n# le_soil = LabelEncoder().fit(train['Soil Type'])\n# le_crop = LabelEncoder().fit(train['Crop Type'])\n# train['Soil_le'] = le_soil.transform(train['Soil Type'])\n# train['Crop_le'] = le_crop.transform(train['Crop Type'])\n# test['Soil_le']  = le_soil.transform(test['Soil Type'])\n# test['Crop_le']  = le_crop.transform(test['Crop Type'])\n\n# train['Soil_Crop_inter'] = train['Soil_le'] * 100 + train['Crop_le']\n# test['Soil_Crop_inter']  = test['Soil_le']  * 100 + test['Crop_le']\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 🗒️ 6. Prepare Feature Matrix\n# feature_cols = [\n#     'Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous',\n#     'Soil Type_te','Crop Type_te',\n#     'N_K_ratio','T_minus_H','Soil_Crop_inter'\n# ]\n# X = train[feature_cols]\n# y = train['target_enc'].values\n# X_test = test[feature_cols]\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 🎯 7. Initial LightGBM Training (for pseudo-labels)\n# X_tr, X_val, y_tr, y_val = train_test_split(\n#     X, y, stratify=y, test_size=0.1, random_state=42\n# )\n# lgb_model = lgb.LGBMClassifier(\n#     objective='multiclass',\n#     num_class=len(le_target.classes_),\n#     learning_rate=0.1,\n#     n_estimators=500,\n#     random_state=42,\n#     verbose=-1\n# )\n# lgb_model.fit(\n#     X_tr, y_tr,\n#     eval_set=[(X_val, y_val)],\n#     callbacks=[\n#         lgb.early_stopping(stopping_rounds=50),\n#         lgb.log_evaluation(period=100)  # change 100 to 0 if you want no logging\n#     ]\n# )\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 🧪 8. Pseudo-Labeling High-Confidence Test Rows\n# probs_test = lgb_model.predict_proba(X_test)\n# conf_mask = (probs_test.max(axis=1) >= 0.80)\n# X_pseudo = X_test[conf_mask]\n# y_pseudo = np.argmax(probs_test[conf_mask], axis=1)\n\n# # Combine original + pseudo\n# X_ext = pd.concat([X, X_pseudo], ignore_index=True)\n# y_ext = np.concatenate([y, y_pseudo])\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 🏋️ 9. Retrain Models on Extended Data\n\n# # 9.1 LightGBM on extended data\n# lgb_final = lgb.LGBMClassifier(\n#     objective='multiclass',\n#     num_class=len(le_target.classes_),\n#     learning_rate=0.05,\n#     n_estimators=800,\n#     random_state=2025,\n#     verbose=-1\n# )\n# lgb_final.fit(\n#     X_ext, y_ext,\n#     eval_set=[(X_ext, y_ext)],\n#      callbacks=[\n#         lgb.early_stopping(stopping_rounds=50),\n#     ]\n# )\n\n# # 9.2 CatBoost on extended data\n# cat_model = CatBoostClassifier(\n#     iterations=800,\n#     learning_rate=0.05,\n#     depth=6,\n#     loss_function='MultiClass',\n#     eval_metric='MultiClass',\n#     random_seed=2025,\n#     verbose=100\n# )\n# cat_model.fit(\n#     X_ext, y_ext,\n#     cat_features=[]  # all features are numeric/encoded\n# )\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 🔄 10. Blend & Predict Top-3\n# proba_lgb = lgb_final.predict_proba(X_test)\n# proba_cat = cat_model.predict_proba(X_test)\n# final_proba = 0.7 * proba_lgb + 0.3 * proba_cat\n\n# top3_idx    = np.argsort(final_proba, axis=1)[:, -3:][:, ::-1]\n# top3_labels = le_target.inverse_transform(top3_idx.ravel()).reshape(top3_idx.shape)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 📤 11. Submission\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in top3_labels]\n# })\n# submission.to_csv('submission_pseudo_sm_te_inter_blend.csv', index=False)\n# print(\"✅ submission_pseudo_sm_te_inter_blend.csv saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hybrid TabNet + LightGBM Ensemble with Stratified K-Fold","metadata":{}},{"cell_type":"code","source":"# # Let's generate the complete corrected code for the hybrid TabNet + LightGBM ensemble\n# # including installation, preprocessing, training, and prediction in a Kaggle-compatible way.\n\n# # 📌 1. Imports\n# import numpy as np\n# import pandas as pd\n# from sklearn.preprocessing import LabelEncoder, StandardScaler\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.metrics import log_loss\n# import lightgbm as lgb\n# from pytorch_tabnet.tab_model import TabNetClassifier\n# import torch\n# import warnings\n# from lightgbm import early_stopping, log_evaluation\n\n# warnings.filterwarnings(\"ignore\")\n\n# # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n# # 📌 2. Load & Preprocess Data\n# train = pd.read_csv(\"/kaggle/input/playground-series-s5e6/train.csv\")\n# test  = pd.read_csv(\"/kaggle/input/playground-series-s5e6/test.csv\")\n\n# # Fix typo\n# train.rename(columns={\"Temparature\": \"Temperature\"}, inplace=True)\n# test.rename(columns={\"Temparature\": \"Temperature\"}, inplace=True)\n\n# # Label encode target\n# le = LabelEncoder()\n# train[\"target\"] = le.fit_transform(train[\"Fertilizer Name\"])\n# target = \"target\"\n\n# # Feature columns\n# cat_cols = [\"Soil Type\", \"Crop Type\"]\n# num_cols = [\"Temperature\", \"Humidity\", \"Moisture\", \"Nitrogen\", \"Potassium\", \"Phosphorous\"]\n# features = num_cols + cat_cols\n\n# # Combine data for consistent encoding\n# train[\"is_train\"] = 1\n# test[\"is_train\"] = 0\n# full_df = pd.concat([train[features + [\"is_train\"]], test[features + [\"is_train\"]]])\n\n# # Encode categoricals\n# for col in cat_cols:\n#     le_col = LabelEncoder()\n#     full_df[col] = le_col.fit_transform(full_df[col])\n\n# # Split back\n# train_df = full_df[full_df.is_train == 1].drop(\"is_train\", axis=1).copy()\n# test_df  = full_df[full_df.is_train == 0].drop(\"is_train\", axis=1).copy()\n# X = train_df[features]\n# y = train[target]\n# X_test = test_df[features]\n# n_classes = y.nunique()\n\n# # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n# # 📌 3. Training with Stratified K-Fold (TabNet + LGB)\n# N_SPLITS = 5\n# skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\n# oof_tabnet = np.zeros((len(X), n_classes))\n# oof_lgb    = np.zeros((len(X), n_classes))\n# preds_tabnet = np.zeros((len(X_test), n_classes))\n# preds_lgb    = np.zeros((len(X_test), n_classes))\n\n# for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n#     print(f\"🔁 Fold {fold}\")\n#     X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n#     y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n#     # ➤ LightGBM\n#     lgb_model = lgb.LGBMClassifier(\n#         objective=\"multiclass\",\n#         num_class=n_classes,\n#         learning_rate=0.05,\n#         n_estimators=500,\n#         random_state=fold,\n#         force_row_wise=True  # efficient row-wise processing\n#     )\n#     lgb_model.fit(\n#     X_tr, y_tr,\n#     eval_set=[(X_val, y_val)],\n#     callbacks=[\n#         early_stopping(stopping_rounds=50),\n#         log_evaluation(period=0)  # This replaces the need for verbose\n#     ]\n# )\n#     oof_lgb[val_idx] = lgb_model.predict_proba(X_val)\n#     preds_lgb += lgb_model.predict_proba(X_test) / N_SPLITS\n\n#     # ➤ TabNet\n#     tabnet = TabNetClassifier(\n#         n_d=16, n_a=16,\n#         n_steps=5,\n#         gamma=1.5,\n#         n_independent=2,\n#         n_shared=2,\n#         optimizer_fn=torch.optim.Adam,\n#         optimizer_params=dict(lr=2e-2),\n#         mask_type='entmax',\n#         verbose=0,\n#         seed=fold\n#     )\n#     tabnet.fit(\n#         X_tr.values, y_tr.values,\n#         eval_set=[(X_val.values, y_val.values)],\n#         eval_metric=[\"logloss\"],\n#         max_epochs=200,\n#         patience=20,\n#         batch_size=1024,\n#         virtual_batch_size=128,\n#         num_workers=0,\n#         drop_last=False\n#     )\n#     oof_tabnet[val_idx] = tabnet.predict_proba(X_val.values)\n#     preds_tabnet += tabnet.predict_proba(X_test.values) / N_SPLITS\n\n# # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n# # 📌 4. Blending Predictions\n# oof_blend = 0.6 * oof_lgb + 0.4 * oof_tabnet\n# final_preds = 0.6 * preds_lgb + 0.4 * preds_tabnet\n# print(\"📉 OOF LogLoss:\", log_loss(y, oof_blend))\n\n# # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n# # 📌 5. Prepare Submission\n# top3 = np.argsort(final_preds, axis=1)[:, -3:][:, ::-1]\n# top3_labels = le.inverse_transform(top3.ravel()).reshape(-1, 3)\n# submission = pd.DataFrame({\n#     \"id\": test[\"id\"],\n#     \"Fertilizer Name\": [\" \".join(row) for row in top3_labels]\n# })\n# submission.to_csv(\"submission_tabnet_lgb.csv\", index=False)\n# print(\"✅ submission_tabnet_lgb.csv saved.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Install dependencies (run this cell once at the top)\n# !pip install lightgbm --quiet\n# !pip install pytorch-tabnet --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.metrics import log_loss\n# from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n# from pytorch_tabnet.tab_model import TabNetClassifier\n# import torch\n\n# # 1️⃣ Load & Prepare Data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n# sub = pd.read_csv('/kaggle/input/playground-series-s5e6/sample_submission.csv')\n\n# # Fix typo\n# train.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n\n# # Encode Target\n# le = LabelEncoder()\n# train['target'] = le.fit_transform(train['Fertilizer Name'])\n# n_classes = train['target'].nunique()\n\n# # Encode categorical features\n# cat_cols = ['Soil Type', 'Crop Type']\n# for col in cat_cols:\n#     le_col = LabelEncoder()\n#     train[col] = le_col.fit_transform(train[col])\n#     test[col] = le_col.transform(test[col])\n\n# # Feature list\n# features = ['Soil Type', 'Crop Type', 'Temperature', 'Humidity', \n#             'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']\n\n# X = train[features].copy()\n# y = train['target'].copy()\n# X_test = test[features].copy()\n\n# # 2️⃣ TabNet + LightGBM Ensemble\n# N_SPLITS = 5\n# oof_tabnet = np.zeros((len(train), n_classes))\n# oof_lgb = np.zeros((len(train), n_classes))\n# preds_tabnet = np.zeros((len(test), n_classes))\n# preds_lgb = np.zeros((len(test), n_classes))\n\n# skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\n# for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y)):\n#     print(f\"📁 Fold {fold + 1}\")\n\n#     X_tr, X_val = X.iloc[tr_idx].values, X.iloc[val_idx].values\n#     y_tr, y_val = y.iloc[tr_idx].values, y.iloc[val_idx].values\n\n#     # ➤ TabNet\n#     tabnet = TabNetClassifier(\n#         seed=42,\n#         verbose=0,\n#         device_name='cuda' if torch.cuda.is_available() else 'cpu'\n#     )\n#     tabnet.fit(\n#         X_tr, y_tr,\n#         eval_set=[(X_val, y_val)],\n#         eval_name=[\"val\"],\n#         eval_metric=[\"logloss\"],\n#         patience=20,\n#         max_epochs=200,\n#         batch_size=1024,\n#         virtual_batch_size=128\n#     )\n#     oof_tabnet[val_idx] = tabnet.predict_proba(X_val)\n#     preds_tabnet += tabnet.predict_proba(X_test.values) / N_SPLITS\n\n#     # ➤ LightGBM\n#     lgb_model = LGBMClassifier(\n#         objective=\"multiclass\",\n#         num_class=n_classes,\n#         learning_rate=0.05,\n#         n_estimators=500,\n#         random_state=fold,\n#         force_row_wise=True\n#     )\n#     lgb_model.fit(\n#         X.iloc[tr_idx], y.iloc[tr_idx],\n#         eval_set=[(X.iloc[val_idx], y.iloc[val_idx])],\n#         callbacks=[\n#             early_stopping(stopping_rounds=50),\n#             log_evaluation(period=0)\n#         ]\n#     )\n#     oof_lgb[val_idx] = lgb_model.predict_proba(X.iloc[val_idx])\n#     preds_lgb += lgb_model.predict_proba(X_test) / N_SPLITS\n\n# # 3️⃣ Ensemble Predictions\n# # Weighted average (adjust these if desired)\n# final_preds = 0.55 * preds_tabnet + 0.45 * preds_lgb\n\n# # 4️⃣ Create Submission File\n# top3 = np.argsort(final_preds, axis=1)[:, -3:][:, ::-1]\n# top3_labels = le.inverse_transform(top3.ravel()).reshape(-1, 3)\n# sub['Fertilizer Name'] = [' '.join(row) for row in top3_labels]\n# sub.to_csv(\"submission_tabnet_lgbm_ensemble.csv\", index=False)\n# print(\"✅ submission_tabnet_lgbm_ensemble.csv saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# H2O method","metadata":{}},{"cell_type":"code","source":"# # 📦 1. Install and import\n# !pip install -q h2o","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import h2o\n# from h2o.automl import H2OAutoML\n# import pandas as pd\n# import numpy as np\n\n# # 🔄 2. Initialize H2O\n# h2o.init()\n\n# # 📂 3. Load Data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # 🧼 4. Fix column typo (just in case)\n# train.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n\n# # 🏷️ 5. Target Encoding\n# target = 'Fertilizer Name'\n# id_col = 'id'\n# features = [col for col in train.columns if col not in [target, id_col]]\n\n# # Convert to H2OFrames\n# train_h2o = h2o.H2OFrame(train[features + [target]])\n# test_h2o = h2o.H2OFrame(test[features])\n\n# # 🔮 6. Run AutoML\n# aml = H2OAutoML(max_models=20, seed=42, sort_metric='mean_per_class_error')\n# aml.train(x=features, y=target, training_frame=train_h2o)\n\n# # ✅ 7. Predict on test\n# preds = aml.leader.predict(test_h2o)\n# preds_df = preds.as_data_frame()\n\n# # 🥇 8. Create submission with top 3 predictions\n# top3 = preds_df[['predict', 'p1', 'p2', 'p3']]\n# top3_preds = preds_df.drop('predict', axis=1).apply(\n#     lambda row: ' '.join(preds.columns[1:][row.values.argsort()[::-1][:3]]), axis=1)\n\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': top3_preds\n# })\n# submission.to_csv('submission_h2o.csv', index=False)\n# print(\"✅ submission_h2o.csv saved.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CatBoost-based pipeline","metadata":{}},{"cell_type":"code","source":"# # 📦 1. Imports\n# import pandas as pd\n# import numpy as np\n# from catboost import CatBoostClassifier, Pool\n# from sklearn.model_selection import train_test_split\n\n# # 📂 2. Load Data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # 🔧 3. Target & Categorical Columns\n# target_col = \"Fertilizer Name\"\n# cat_cols = ['Soil Type', 'Crop Type']\n# num_cols = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']\n# features = num_cols + cat_cols\n\n# # 🧪 4. Split for validation (optional, for local tuning)\n# X_train, X_val, y_train, y_val = train_test_split(\n#     train[features], train[target_col], test_size=0.1, random_state=42, stratify=train[target_col]\n# )\n\n# # 🧠 5. CatBoost Model Setup\n# model = CatBoostClassifier(\n#     iterations=3000,\n#     learning_rate=0.03,\n#     depth=7,\n#     eval_metric='MultiClass',\n#     loss_function='MultiClass',\n#     cat_features=cat_cols,\n#     verbose=200,\n#     early_stopping_rounds=100,\n#     random_state=42\n# )\n\n# # 🏋️ 6. Train Model\n# train_pool = Pool(X_train, y_train, cat_features=cat_cols)\n# val_pool = Pool(X_val, y_val, cat_features=cat_cols)\n# model.fit(train_pool, eval_set=val_pool)\n\n# # 📊 7. Predict Top-3 Labels\n# test_pool = Pool(test[features], cat_features=cat_cols)\n# probs = model.predict_proba(test_pool)\n# labels = model.classes_\n\n# top3 = np.argsort(probs, axis=1)[:, -3:][:, ::-1]\n# top3_labels = labels[top3]\n\n# # 📝 8. Submission Format\n# submission = pd.DataFrame({\n#     \"id\": test[\"id\"],\n#     \"Fertilizer Name\": [' '.join(row) for row in top3_labels]\n# })\n\n# submission.to_csv(\"submission_catboost.csv\", index=False)\n# print(\"✅ submission_catboost.csv saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## another catboots based model","metadata":{}},{"cell_type":"code","source":"# # 📦 Install CatBoost if needed (run once at top)\n# # !pip install catboost --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# import pandas as pd\n# import numpy as np\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import StratifiedKFold\n# from catboost import CatBoostClassifier, Pool\n\n# # ─────────────────────────────────────────────────────────\n# # 1️⃣ Load & Prepare Data\n# df = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv', index_col='id')\n# test = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix typo\n# df.rename(columns={'Temparature':'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature':'Temperature'}, inplace=True)\n\n# # Separate X and y\n# X = df.drop(columns=['Fertilizer Name'])\n# y = df['Fertilizer Name']\n\n# # Label-encode the target\n# le_target = LabelEncoder()\n# y_enc = le_target.fit_transform(y)\n\n# # Features\n# cat_cols = ['Soil Type','Crop Type']\n# num_cols = ['Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous']\n# features = num_cols + cat_cols\n\n# # We'll train on DataFrames for CatBoost\n# X = X[features].copy()\n# test = test[features].copy()\n\n# # ─────────────────────────────────────────────────────────\n# # helper: MAP@3\n# def mapk(actual, predicted, k=3):\n#     def apk(a, p):\n#         p = p[:k]\n#         score = 0.0\n#         hits = 0\n#         seen = set()\n#         for i, pred in enumerate(p):\n#             if pred in a and pred not in seen:\n#                 hits += 1\n#                 score += hits / (i+1.0)\n#                 seen.add(pred)\n#         return score / min(len(a), k)\n#     return np.mean([apk(a, p) for a, p in zip(actual, predicted)])\n\n# # ─────────────────────────────────────────────────────────\n# # 2️⃣ Try different expansion counts\n# results = []\n# ocount = 0  # no external data in this run\n\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# for count in range(1, 6):  # try counts 1 through 5\n#     print(f\"\\n▶▶▶ Expansion count = {count}\")\n#     fold_scores = []\n    \n#     # prepare external placeholders (empty)\n#     xo = pd.DataFrame(columns=features)\n#     yo = np.array([])\n\n#     # OOF predictions just to compute MAP@3\n#     oof_preds = np.zeros((len(X), 3), dtype=object)  # store top3 lists\n\n#     for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y_enc), 1):\n#         # split\n#         X_tr = X.iloc[tr_idx].reset_index(drop=True)\n#         y_tr = pd.Series(y_enc[tr_idx])\n#         X_val = X.iloc[val_idx]\n#         y_val = y_enc[val_idx]\n\n#         # 2.1 expand\n#         X_tr_ext = pd.concat([X_tr]*count + [xo]*ocount, ignore_index=True)\n#         y_tr_ext = pd.concat([y_tr]*count + [pd.Series(yo)*ocount], ignore_index=True)\n\n#         # 2.2 train CatBoost\n#         model = CatBoostClassifier(\n#             iterations=500,\n#             learning_rate=0.1,\n#             depth=6,\n#             loss_function='MultiClass',\n#             eval_metric='MultiClass',\n#             cat_features=[X_tr_ext.columns.get_loc(c) for c in cat_cols],\n#             task_type='CPU', verbose=0\n#         )\n#         model.fit(X_tr_ext, y_tr_ext)\n\n#         # 2.3 predict val\n#         probas = model.predict_proba(X_val)\n#         top3_idx = np.argsort(probas, axis=1)[:, -3:][:, ::-1]\n#         top3_labels = le_target.inverse_transform(top3_idx.ravel()).reshape(top3_idx.shape)\n\n#         # compute MAP@3 on this fold\n#         actual = [[lab] for lab in y_val]\n#         fold_score = mapk(actual, top3_labels.tolist(), k=3)\n#         fold_scores.append(fold_score)\n\n#         print(f\"  Fold {fold} MAP@3 = {fold_score:.4f}\")\n\n#     mean_score = np.mean(fold_scores)\n#     print(f\"→ Expansion count {count} mean MAP@3 = {mean_score:.4f}\")\n#     results.append((count, mean_score))\n\n# # ─────────────────────────────────────────────────────────\n# # 3️⃣ Show best count\n# best_count, best_score = max(results, key=lambda x: x[1])\n# print(f\"\\n🎉 Best expansion count = {best_count} with MAP@3 = {best_score:.4f}\")\n\n# # ─────────────────────────────────────────────────────────\n# # 4️⃣ Train final model on full expanded set and predict test\n\n# # Expand full training set\n# X_full_ext = pd.concat([X]*best_count, ignore_index=True)\n# y_full_ext = np.tile(y_enc, best_count)\n\n# # Train final\n# final_model = CatBoostClassifier(\n#     iterations=500,\n#     learning_rate=0.1,\n#     depth=6,\n#     loss_function='MultiClass',\n#     eval_metric='MultiClass',\n#     cat_features=[X_full_ext.columns.get_loc(c) for c in cat_cols],\n#     task_type='CPU',\n#     verbose=100\n# )\n# final_model.fit(X_full_ext, y_full_ext)\n\n# # Predict test\n# probas_test = final_model.predict_proba(test)\n# top3_idx = np.argsort(probas_test, axis=1)[:, -3:][:, ::-1]\n# top3_labels = le_target.inverse_transform(top3_idx.ravel()).reshape(top3_idx.shape)\n\n# # Submission\n# submission = pd.DataFrame({\n#     'id': test.index,\n#     'Fertilizer Name': [' '.join(row) for row in top3_labels]\n# })\n# submission.to_csv('submission_expansion.csv', index=False)\n# print(\"✅ submission_expansion.csv saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LightGBM-only pipeline","metadata":{}},{"cell_type":"code","source":"!pip install lightgbm --quiet\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # LightGBM-Only Fast Baseline for Fertilizer Recommendation (Kaggle Playground S5E6)\n\n# import pandas as pd\n# import lightgbm as lgb\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import train_test_split\n# from lightgbm import LGBMClassifier\n# import numpy as np\n\n# # Load data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix column name typo if exists\n# train.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n\n# # Encode target\n# le = LabelEncoder()\n# train['target'] = le.fit_transform(train['Fertilizer Name'])\n# n_classes = len(le.classes_)\n\n# # Target mean encoding for categoricals\n# for col in ['Soil Type', 'Crop Type']:\n#     means = train.groupby(col)['target'].mean()\n#     train[f'{col}_enc'] = train[col].map(means)\n#     test[f'{col}_enc'] = test[col].map(means)\n\n# # Define features\n# features = ['Temperature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous',\n#             'Soil Type_enc', 'Crop Type_enc']\n# X = train[features]\n# y = train['target']\n# X_test = test[features]\n\n# # Train/validation split\n# X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\n# # LightGBM dataset\n# train_set = lgb.Dataset(X_train, label=y_train)\n# val_set = lgb.Dataset(X_val, label=y_val)\n\n# # LightGBM params\n# params = {\n#     'objective': 'multiclass',\n#     'num_class': n_classes,\n#     'metric': 'multi_logloss',\n#     'learning_rate': 0.1,\n#     'verbosity': -1,\n#     'seed': 42\n# }\n\n# callbacks = [\n#     lgb.early_stopping(stopping_rounds=50),\n#     lgb.log_evaluation(period=100)\n# ]\n\n# model = lgb.train(\n#     params,\n#     train_set,\n#     num_boost_round=10000,\n#     valid_sets=[train_set, val_set],\n#     callbacks=callbacks\n# )\n\n# # Predict\n# probs = model.predict(X_test)\n# top3 = np.argsort(probs, axis=1)[:, -3:][:, ::-1]\n# top3_labels = le.inverse_transform(top3.ravel()).reshape(top3.shape)\n\n# # Build submission\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in top3_labels]\n# })\n# submission.to_csv('submission_lgbm_fast.csv', index=False)\n\n# print(\"✅ submission_lgbm_fast.csv saved.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LightGBM + Bagging","metadata":{}},{"cell_type":"code","source":"# # 📦 Install dependencies (run once at top)\n# !pip install lightgbm optuna --quiet\n\n# # ───────────────────────────────────────────────────────────────────────────────","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 1️⃣ Imports\n# import pandas as pd\n# import numpy as np\n# import lightgbm as lgb\n# import optuna\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.metrics import log_loss\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 2️⃣ Load & preprocess data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix typo\n# train.rename(columns={'Temparature':'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature':'Temperature'}, inplace=True)\n\n# # Encode target\n# le = LabelEncoder()\n# train['target'] = le.fit_transform(train['Fertilizer Name'])\n# n_classes = train['target'].nunique()\n\n# # Target‐mean encoding for categoricals\n# for col in ['Soil Type','Crop Type']:\n#     m = train.groupby(col)['target'].mean()\n#     train[f'{col}_enc'] = train[col].map(m)\n#     test[f'{col}_enc']  = test[col].map(m)\n\n# # Features\n# features = ['Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous',\n#             'Soil Type_enc','Crop Type_enc']\n# X = train[features].values\n# y = train['target'].values\n# X_test = test[features].values\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 3️⃣ Optuna objective to tune LightGBM\n# def objective(trial):\n#     param = {\n#         'objective': 'multiclass',\n#         'num_class': n_classes,\n#         'metric': 'multi_logloss',\n#         'verbosity': -1,\n#         'boosting_type': 'gbdt',\n#         'learning_rate': trial.suggest_float('lr', 0.01, 0.3, log=True),\n#         'num_leaves': trial.suggest_int('num_leaves', 16, 256),\n#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 200),\n#         'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n#         'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 10.0),\n#         'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 10.0),\n#     }\n#     # 3‐fold stratified CV\n#     skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n#     losses = []\n#     for train_idx, val_idx in skf.split(X, y):\n#         dtrain = lgb.Dataset(X[train_idx], label=y[train_idx])\n#         dval   = lgb.Dataset(X[val_idx], label=y[val_idx])\n#         gbm = lgb.train(\n#             param, dtrain,\n#             num_boost_round=1000,\n#             valid_sets=[dval],\n#             callbacks=[\n#                 lgb.early_stopping(stopping_rounds=50),\n#                 lgb.log_evaluation(period=0)\n#             ],\n#         )\n#         preds = gbm.predict(X[val_idx])\n#         losses.append(log_loss(y[val_idx], preds))\n#     return np.mean(losses)\n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=40, timeout=600)\n# best_params = study.best_params\n# print(\"Best params:\", best_params)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 4️⃣ Bagged predictions with tuned params\n# BAGS = 5\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# test_preds = np.zeros((X_test.shape[0], n_classes))\n\n# for bag in range(BAGS):\n#     print(f\"Bag {bag+1}/{BAGS}\")\n#     seed = 1000 + bag\n#     preds_oof = np.zeros_like(test_preds)\n\n#     for train_idx, val_idx in skf.split(X, y):\n#         dtrain = lgb.Dataset(X[train_idx], label=y[train_idx])\n#         dval = lgb.Dataset(X[val_idx], label=y[val_idx])\n\n#         gbm = lgb.train(\n#             {**best_params, 'seed': seed, 'objective': 'multiclass', 'num_class': n_classes, 'metric': 'multi_logloss'},\n#             dtrain,\n#             num_boost_round=1000,\n#             valid_sets=[dval],\n#             callbacks=[lgb.early_stopping(50)],\n#         )\n\n#         preds_oof += gbm.predict(X_test) / skf.n_splits\n\n#     test_preds += preds_oof / BAGS\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 5️⃣ Create Submission\n# top3 = np.argsort(test_preds, axis=1)[:, -3:][:, ::-1]\n# labels = le.inverse_transform(top3.ravel()).reshape(top3.shape)\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in labels]\n# })\n# submission.to_csv('submission_optuna_lgbm_bag.csv', index=False)\n# print(\"✅ submission_optuna_lgbm_bag.csv saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Base + Meta Neural Net","metadata":{}},{"cell_type":"code","source":"# # Full End‑to‑End Stacking Pipeline with Feature Engineering, Calibrated Base Models, and Neural Meta-Learner\n\n# import pandas as pd\n# import numpy as np\n# from sklearn.model_selection import StratifiedKFold, train_test_split\n# from sklearn.preprocessing import LabelEncoder, PolynomialFeatures, StandardScaler\n# from sklearn.calibration import CalibratedClassifierCV\n# from sklearn.metrics import log_loss\n# import lightgbm as lgb\n# import xgboost as xgb\n# import catboost as cb\n# import tensorflow as tf\n# from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.optimizers import Adam\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 1️⃣ Load data\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix typo\n# train.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n# test .rename(columns={'Temparature': 'Temperature'}, inplace=True)\n\n# # Separate target\n# le_target = LabelEncoder()\n# train['target'] = le_target.fit_transform(train['Fertilizer Name'])\n\n# # Feature columns\n# NUM_COLS = ['Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous']\n# CAT_COLS = ['Soil Type','Crop Type']\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 2️⃣ Feature engineering\n\n# # 2.1: Polynomial features (degree 2 interactions)\n# poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n# poly_feats_train = poly.fit_transform(train[NUM_COLS])\n# poly_feats_test  = poly.transform(test[NUM_COLS])\n# poly_names = poly.get_feature_names_out(NUM_COLS)\n\n# df_poly_train = pd.DataFrame(poly_feats_train, columns=poly_names, index=train.index)\n# df_poly_test  = pd.DataFrame(poly_feats_test,  columns=poly_names, index=test.index)\n\n# # 2.2: Target-mean encoding for categoricals\n# soil_map = train.groupby('Soil Type')['target'].mean()\n# crop_map = train.groupby('Crop Type')['target'].mean()\n# train['Soil_TE'] = train['Soil Type'].map(soil_map)\n# test['Soil_TE']  = test['Soil Type'].map(soil_map)\n# train['Crop_TE'] = train['Crop Type'].map(crop_map)\n# test['Crop_TE']  = test['Crop Type'].map(crop_map)\n\n# # 2.3: Label encode categoricals for base models\n# le_soil = LabelEncoder().fit(train['Soil Type'])\n# le_crop = LabelEncoder().fit(train['Crop Type'])\n# train['Soil_LE'] = le_soil.transform(train['Soil Type'])\n# train['Crop_LE'] = le_crop.transform(train['Crop Type'])\n# test['Soil_LE']  = le_soil.transform(test['Soil Type'])\n# test['Crop_LE']  = le_crop.transform(test['Crop Type'])\n\n# # 2.4: Assemble final numeric feature matrices\n# BASE_NUMERIC = NUM_COLS + ['Soil_LE','Crop_LE','Soil_TE','Crop_TE']\n# X_base_train = train[BASE_NUMERIC].copy()\n# X_base_test  = test[BASE_NUMERIC].copy()\n\n# # Combine with polynomial features\n# X_base_train = pd.concat([X_base_train, df_poly_train], axis=1)\n# X_base_test  = pd.concat([X_base_test,  df_poly_test], axis=1)\n\n# # Remove duplicate columns\n# X_base_train = X_base_train.loc[:, ~X_base_train.columns.duplicated()]\n# X_base_test  = X_base_test.loc[:, ~X_base_test.columns.duplicated()]\n\n# y = train['target'].values\n# n_classes = len(le_target.classes_)\n\n# # Optional: scale numeric features for neural meta model later\n# scaler = StandardScaler().fit(X_base_train)\n# X_scaled_train = scaler.transform(X_base_train)\n# X_scaled_test  = scaler.transform(X_base_test)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 3️⃣ Generate calibrated OOF predictions for base models\n\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# oof_preds = {\n#     'lgb': np.zeros((len(train), n_classes)),\n#     'xgb': np.zeros((len(train), n_classes)),\n#     'cat': np.zeros((len(train), n_classes))\n# }\n# test_preds = {\n#     'lgb': np.zeros((len(test), n_classes)),\n#     'xgb': np.zeros((len(test), n_classes)),\n#     'cat': np.zeros((len(test), n_classes))\n# }\n\n# for fold, (tr_idx, val_idx) in enumerate(skf.split(X_base_train, y), 1):\n#     X_tr, X_val = X_base_train.iloc[tr_idx], X_base_train.iloc[val_idx]\n#     y_tr, y_val = y[tr_idx], y[val_idx]\n    \n#     # LightGBM\n#     m_lgb = lgb.LGBMClassifier(random_state=fold)\n#     m_lgb.fit(X_tr, y_tr)\n#     cal_lgb = CalibratedClassifierCV(m_lgb, cv=3, method='sigmoid')\n#     cal_lgb.fit(X_val, y_val)\n#     oof_preds['lgb'][val_idx] = cal_lgb.predict_proba(X_val)\n#     test_preds['lgb'] += cal_lgb.predict_proba(X_base_test) / skf.n_splits\n\n#     # XGBoost\n#     m_xgb = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=fold)\n#     m_xgb.fit(X_tr, y_tr)\n#     cal_xgb = CalibratedClassifierCV(m_xgb, cv=3, method='sigmoid')\n#     cal_xgb.fit(X_val, y_val)\n#     oof_preds['xgb'][val_idx] = cal_xgb.predict_proba(X_val)\n#     test_preds['xgb'] += cal_xgb.predict_proba(X_base_test) / skf.n_splits\n\n#     # CatBoost\n#     m_cat = cb.CatBoostClassifier(verbose=0, random_state=fold)\n#     m_cat.fit(X_tr, y_tr)\n#     cal_cat = CalibratedClassifierCV(m_cat, cv=3, method='sigmoid')\n#     cal_cat.fit(X_val, y_val)\n#     oof_preds['cat'][val_idx] = cal_cat.predict_proba(X_val)\n#     test_preds['cat'] += cal_cat.predict_proba(X_base_test) / skf.n_splits\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 4️⃣ Prepare meta‑model inputs\n\n# # OOF stacked probabilities\n# X_meta_oof  = np.hstack([oof_preds[m] for m in ['lgb','xgb','cat']])\n# X_meta_test = np.hstack([test_preds[m]   for m in ['lgb','xgb','cat']])\n\n# # Also include scaled original features for meta model\n# X_meta_oof  = np.hstack([X_meta_oof,  X_scaled_train])\n# X_meta_test = np.hstack([X_meta_test, X_scaled_test])\n\n# # Convert y to categorical for meta NN\n# y_cat_oof = tf.keras.utils.to_categorical(y, num_classes=n_classes)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 5️⃣ Build & train neural network meta‑learner\n\n# input_dim = X_meta_oof.shape[1]\n# inp = Input(shape=(input_dim,), name='meta_input')\n# x = Dense(64, activation='relu')(inp)\n# x = BatchNormalization()(x)\n# x = Dropout(0.3)(x)\n# x = Dense(32, activation='relu')(x)\n# x = BatchNormalization()(x)\n# x = Dropout(0.2)(x)\n# out = Dense(n_classes, activation='softmax')(x)\n\n# meta_model = Model(inp, out)\n# meta_model.compile(\n#     optimizer=Adam(learning_rate=1e-3),\n#     loss='categorical_crossentropy'\n# )\n\n# # Train/validation split for meta model\n# X_tr_meta, X_val_meta, y_tr_meta, y_val_meta = train_test_split(\n#     X_meta_oof, y_cat_oof, test_size=0.2, random_state=42, stratify=y\n# )\n\n# meta_model.fit(\n#     X_tr_meta, y_tr_meta,\n#     validation_data=(X_val_meta, y_val_meta),\n#     epochs=20,\n#     batch_size=512,\n#     verbose=2\n# )\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 6️⃣ Predict final and build submission\n\n# meta_probs = meta_model.predict(X_meta_test)\n# top3_idx = np.argsort(meta_probs, axis=1)[:, -3:][:, ::-1]\n# top3_labels = le_target.inverse_transform(top3_idx.ravel()).reshape(top3_idx.shape)\n\n# submission = pd.DataFrame({\n#     'id': test['id'],\n#     'Fertilizer Name': [' '.join(row) for row in top3_labels]\n# })\n# submission.to_csv('submission_stacked_nn.csv', index=False)\n# print(\"✅ submission_stacked_nn.csv saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Wide & Deep model in a TPUStrategy","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import tensorflow as tf\n# from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, Flatten\n# from tensorflow.keras.models import Model\n# from sklearn.preprocessing import LabelEncoder, StandardScaler\n# from sklearn.model_selection import train_test_split\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 1️⃣ Load & Preprocess Data\n# df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/train.csv\")\n# test_df = pd.read_csv(\"/kaggle/input/playground-series-s5e6/test.csv\")\n\n# # Fix typo\n# df.rename(columns={'Temparature':'Temperature'}, inplace=True)\n# test_df.rename(columns={'Temparature':'Temperature'}, inplace=True)\n\n# # Encode target\n# le_target = LabelEncoder()\n# df['target'] = le_target.fit_transform(df['Fertilizer Name'])\n\n# # Feature columns\n# cat_cols  = ['Soil Type', 'Crop Type']\n# cont_cols = ['Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous']\n\n# # Label‑encode categoricals\n# encoders = {}\n# for col in cat_cols:\n#     enc = LabelEncoder().fit(df[col])\n#     df[col]      = enc.transform(df[col])\n#     test_df[col] = enc.transform(test_df[col])\n#     encoders[col] = enc\n\n# # Scale continuous\n# scaler = StandardScaler().fit(df[cont_cols])\n# df[cont_cols]      = scaler.transform(df[cont_cols])\n# test_df[cont_cols] = scaler.transform(test_df[cont_cols])\n\n# # Train/validation split\n# train_df, val_df = train_test_split(\n#     df, test_size=0.2, stratify=df['target'], random_state=42\n# )\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 2️⃣ Build & Compile Wide & Deep Model\n\n# # Categorical inputs & embeddings\n# inputs, embeds = [], []\n# for col in cat_cols:\n#     vocab = df[col].nunique()\n#     inp = Input(shape=(1,), name=f\"{col}_in\")\n#     emb = Embedding(input_dim=vocab+1, output_dim=4)(inp)\n#     emb = Flatten()(emb)\n#     inputs.append(inp)\n#     embeds.append(emb)\n\n# # Continuous input\n# cont_input = Input(shape=(len(cont_cols),), name=\"cont_in\")\n# inputs.append(cont_input)\n\n# # Wide branch\n# wide = Dense(64, activation='relu')(cont_input)\n\n# # Deep branch\n# x = Concatenate()(embeds + [wide])\n# x = Dense(128, activation='relu')(x)\n# x = Dense(64, activation='relu')(x)\n# output = Dense(len(le_target.classes_), activation='softmax')(x)\n\n# model = Model(inputs, output)\n# model.compile(\n#     optimizer='adam',\n#     loss='sparse_categorical_crossentropy',\n#     metrics=['sparse_categorical_accuracy']\n# )\n# model.summary()\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 3️⃣ Prepare tf.data Datasets for Training\n\n# def make_dataset(df_, shuffle=False, batch_size=1024):\n#     cat_arrays = [df_[col].values for col in cat_cols]\n#     cont_array = df_[cont_cols].values\n#     X = tuple(cat_arrays + [cont_array])\n#     y = df_['target'].values\n#     ds = tf.data.Dataset.from_tensor_slices((X, y))\n#     if shuffle:\n#         ds = ds.shuffle(buffer_size=len(df_))\n#     return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n# batch_size = 1024\n# train_ds = make_dataset(train_df, shuffle=True,  batch_size=batch_size)\n# val_ds   = make_dataset(val_df,   shuffle=False, batch_size=batch_size)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 4️⃣ Train the Model\n# model.fit(\n#     train_ds,\n#     validation_data=val_ds,\n#     epochs=20,\n#     verbose=2\n# )\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 5️⃣ Predict & Build Submission (direct NumPy inputs)\n# # Prepare numpy inputs in same order as model inputs\n# test_inputs = [test_df[col].values for col in cat_cols] + [test_df[cont_cols].values]\n\n# # Predict probabilities\n# probs = model.predict(test_inputs, batch_size=batch_size, verbose=0)\n\n# # Extract top‑3 predictions\n# top3_idx    = np.argsort(probs, axis=1)[:, -3:][:, ::-1]\n# top3_labels = le_target.inverse_transform(top3_idx.ravel()).reshape(top3_idx.shape)\n\n# # Create submission\n# submission = pd.DataFrame({\n#     \"id\": test_df[\"id\"],\n#     \"Fertilizer Name\": [' '.join(row) for row in top3_labels]\n# })\n# submission.to_csv(\"submission_wide_deep.csv\", index=False)\n# print(\"✅ submission_wide_deep.csv saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Weighted Ensemble with Optuna‑Tuned Weights + Probability Calibration","metadata":{}},{"cell_type":"code","source":"# !pip install pytorch_tabular --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install \"pytorch_tabular[extra]\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install pytorch-tabular[extra] --quiet\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 📦 Imports\n# import pandas as pd\n# import numpy as np\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.calibration import CalibratedClassifierCV\n# from sklearn.metrics import log_loss\n# import lightgbm as lgb\n# import xgboost as xgb\n# import catboost as cb\n# import optuna\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 1️⃣ Load & Preprocess Data\n# df_train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# df_test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix column typo\n# df_train.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n# df_test.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n\n# # Encode target\n# le = LabelEncoder()\n# df_train['target'] = le.fit_transform(df_train['Fertilizer Name'])\n\n# # Define features\n# features = ['Temperature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']\n# X = df_train[features]\n# y = df_train['target']\n# X_test = df_test[features]\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 2️⃣ Stratified K-Fold Setup\n# n_splits = 5\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n# n_classes = len(le.classes_)\n\n# # Initialize prediction holders\n# oof_lgb = np.zeros((len(X), n_classes))\n# oof_xgb = np.zeros((len(X), n_classes))\n# oof_cat = np.zeros((len(X), n_classes))\n# test_lgb = np.zeros((len(X_test), n_classes))\n# test_xgb = np.zeros((len(X_test), n_classes))\n# test_cat = np.zeros((len(X_test), n_classes))\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 3️⃣ Train each model with calibration\n# for train_idx, val_idx in skf.split(X, y):\n#     X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n#     y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n#     # ✅ LightGBM + Calibration\n#     lgb_model = lgb.LGBMClassifier()\n#     lgb_model.fit(X_tr, y_tr)\n#     cal_lgb = CalibratedClassifierCV(base_estimator=lgb_model, cv=5, method='sigmoid')\n#     cal_lgb.fit(X_val, y_val)\n#     oof_lgb[val_idx] = cal_lgb.predict_proba(X_val)\n#     test_lgb += cal_lgb.predict_proba(X_test) / n_splits\n\n#     # ✅ XGBoost + Calibration\n#     xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n#     xgb_model.fit(X_tr, y_tr)\n#     cal_xgb = CalibratedClassifierCV(base_estimator=xgb_model, cv=5, method='sigmoid')\n#     cal_xgb.fit(X_val, y_val)\n#     oof_xgb[val_idx] = cal_xgb.predict_proba(X_val)\n#     test_xgb += cal_xgb.predict_proba(X_test) / n_splits\n\n#     # ✅ CatBoost + Calibration\n#     cat_model = cb.CatBoostClassifier(verbose=0)\n#     cat_model.fit(X_tr, y_tr)\n#     cal_cat = CalibratedClassifierCV(base_estimator=cat_model, cv=5, method='sigmoid')\n#     cal_cat.fit(X_val, y_val)\n#     oof_cat[val_idx] = cal_cat.predict_proba(X_val)\n#     test_cat += cal_cat.predict_proba(X_test) / n_splits\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 4️⃣ Use Optuna for best ensemble weights\n# def objective(trial):\n#     w1 = trial.suggest_float(\"w_lgb\", 0.0, 1.0)\n#     w2 = trial.suggest_float(\"w_xgb\", 0.0, 1.0)\n#     w3 = trial.suggest_float(\"w_cat\", 0.0, 1.0)\n#     total = w1 + w2 + w3\n#     w1, w2, w3 = w1 / total, w2 / total, w3 / total\n#     oof_combined = w1 * oof_lgb + w2 * oof_xgb + w3 * oof_cat\n#     return log_loss(y, oof_combined)\n\n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials=30)\n\n# best = study.best_params\n# total = best['w_lgb'] + best['w_xgb'] + best['w_cat']\n# w_lgb = best['w_lgb'] / total\n# w_xgb = best['w_xgb'] / total\n# w_cat = best['w_cat'] / total\n# print(\"✅ Optimized Weights — LGB:\", w_lgb, \"XGB:\", w_xgb, \"CAT:\", w_cat)\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # 5️⃣ Final Prediction & Submission\n# test_final = w_lgb * test_lgb + w_xgb * test_xgb + w_cat * test_cat\n\n# top3_idx = np.argsort(test_final, axis=1)[:, -3:][:, ::-1]\n# top3_labels = le.inverse_transform(top3_idx.ravel()).reshape(top3_idx.shape)\n\n# submission = pd.DataFrame({\n#     \"id\": df_test[\"id\"],\n#     \"Fertilizer Name\": [\" \".join(row) for row in top3_labels]\n# })\n# submission.to_csv(\"submission_ensemble.csv\", index=False)\n# print(\"🎉 submission_ensemble.csv saved successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pseudo-Labeled Stacked Ensemble model","metadata":{}},{"cell_type":"code","source":"# # 1. Data Loading & Basic Preprocessing\n# import pandas as pd\n# import numpy as np\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.linear_model import LogisticRegression\n# import lightgbm as lgb\n# import xgboost as xgb\n# import catboost as cb\n\n# # Load and fix typo\n# train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n# train.rename(columns={'Temparature':'Temperature'}, inplace=True)\n# test.rename(columns={'Temparature':'Temperature'},   inplace=True)\n\n# # Encode target and generate simple encodings\n# le_target = LabelEncoder()\n# train['Target'] = le_target.fit_transform(train['Fertilizer Name'])\n\n# for col in ['Soil Type','Crop Type']:\n#     le = LabelEncoder()\n#     train[col] = le.fit_transform(train[col])\n#     test[col]  = le.transform(test[col])\n\n# # Feature set\n# features = ['Temperature','Humidity','Moisture','Nitrogen',\n#             'Potassium','Phosphorous','Soil Type','Crop Type']\n# X = train[features]\n# y = train['Target']\n# X_test = test[features]\n# test_ids = test['id']\n\n\n# # ==== 2. First‑Stage Stacking: OOF + Test Predictions ====\n# n_classes = len(le_target.classes_)\n# n_train = len(train)\n# n_test  = len(test)\n\n# # Prepare arrays\n# oof_preds = {name: np.zeros((n_train, n_classes)) for name in ['lgb','xgb','cat']}\n# test_preds = {name: np.zeros((n_test, n_classes))  for name in ['lgb','xgb','cat']}\n\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# for tr_idx, val_idx in skf.split(X, y):\n#     X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n#     y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n    \n#     # LightGBM\n#     m = lgb.LGBMClassifier()\n#     m.fit(X_tr, y_tr)\n#     oof_preds['lgb'][val_idx] = m.predict_proba(X_val)\n#     test_preds['lgb'] += m.predict_proba(X_test) / skf.n_splits\n    \n#     # XGBoost\n#     m = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n#     m.fit(X_tr, y_tr)\n#     oof_preds['xgb'][val_idx] = m.predict_proba(X_val)\n#     test_preds['xgb'] += m.predict_proba(X_test) / skf.n_splits\n    \n#     # CatBoost\n#     m = cb.CatBoostClassifier(verbose=0)\n#     m.fit(X_tr, y_tr)\n#     oof_preds['cat'][val_idx] = m.predict_proba(X_val)\n#     test_preds['cat'] += m.predict_proba(X_test) / skf.n_splits\n\n# # Stack meta‑features\n# X_meta = np.hstack([oof_preds[m] for m in ['lgb','xgb','cat']])\n# X_test_meta = np.hstack([test_preds[m] for m in ['lgb','xgb','cat']])\n\n# # ==== 3. Pseudo‑Labeling ====\n# # Get high‑confidence test labels (threshold 0.90)\n# conf_thresh = 0.90\n# pseudo_idxs = []\n# pseudo_X = []\n# pseudo_y = []\n\n# # For each test row, if top‑1 prob > thresh, add to pseudo\n# probs = X_test_meta.reshape(n_test, -1)  # meta‑features same shape\n# # But we need original test_preds for labeling\n# avg_probs = (test_preds['lgb'] + test_preds['xgb'] + test_preds['cat'])/3\n# top1 = np.max(avg_probs, axis=1)\n# pred1 = np.argmax(avg_probs, axis=1)\n# for i in range(n_test):\n#     if top1[i] > conf_thresh:\n#         pseudo_idxs.append(i)\n#         pseudo_X.append(X_test_meta[i])\n#         pseudo_y.append(pred1[i])\n\n# # Augment meta-training set\n# if pseudo_X:\n#     X_meta = np.vstack([X_meta, np.array(pseudo_X)])\n#     y_meta = np.concatenate([y, np.array(pseudo_y)])\n# else:\n#     y_meta = y\n\n# # ==== 4. Meta‑Model Training on Augmented Data ====\n# meta = LogisticRegression(multi_class='multinomial', max_iter=1000)\n# meta.fit(X_meta, y_meta)\n\n# # ==== 5. Final Predictions & Submission ====\n# final_probs = meta.predict_proba(X_test_meta)\n# top3 = np.argsort(final_probs, axis=1)[:, -3:][:, ::-1]\n\n# submission = pd.DataFrame({\n#     'id': test_ids,\n#     'Fertilizer Name': [\n#         ' '.join(le_target.inverse_transform(row)) for row in top3\n#     ]\n# })\n# submission.to_csv('submission_pseudo_stacked.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# lightgbm, xgboost, and catboost","metadata":{}},{"cell_type":"code","source":"# Install required libraries for advanced ML models\n!pip install lightgbm xgboost catboost --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## With Folds","metadata":{}},{"cell_type":"code","source":"# # 1. Data Loading and Preprocessing ====\n# import pandas as pd\n# import numpy as np\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.linear_model import LogisticRegression\n\n# # Base model libraries\n# import lightgbm as lgb\n# import xgboost as xgb\n# import catboost as cb\n\n# # Load datasets\n# df_train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# df_test  = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix column typo\n# df_train.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n# df_test.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n\n# # Label‑encode target for numeric mapping\n# le_target = LabelEncoder()\n# df_train['Target'] = le_target.fit_transform(df_train['Fertilizer Name'])\n\n# # Numeric Target Encoding (mean target label) for Soil and Crop\n# soil_mean = df_train.groupby('Soil Type')['Target'].mean()\n# crop_mean = df_train.groupby('Crop Type')['Target'].mean()\n\n# df_train['Soil_TE'] = df_train['Soil Type'].map(soil_mean)\n# df_test['Soil_TE']  = df_test['Soil Type'].map(soil_mean)\n\n# df_train['Crop_TE'] = df_train['Crop Type'].map(crop_mean)\n# df_test['Crop_TE']  = df_test['Crop Type'].map(crop_mean)\n\n# # Label encode categorical features\n# le_soil = LabelEncoder()\n# le_crop = LabelEncoder()\n\n# df_train['Soil'] = le_soil.fit_transform(df_train['Soil Type'])\n# df_test['Soil']  = le_soil.transform(df_test['Soil Type'])\n\n# df_train['Crop'] = le_crop.fit_transform(df_train['Crop Type'])\n# df_test['Crop']  = le_crop.transform(df_test['Crop Type'])\n\n# # Feature list\n# features = ['Temperature','Humidity','Moisture','Nitrogen',\n#             'Potassium','Phosphorous','Soil','Crop','Soil_TE','Crop_TE']\n\n# X = df_train[features].reset_index(drop=True)\n# y = df_train['Target'].reset_index(drop=True)\n# X_test = df_test[features].reset_index(drop=True)\n# test_ids = df_test['id']\n\n# # ==== 2. Prepare Arrays for Stacking ====\n# n_classes = len(le_target.classes_)\n# n_train = X.shape[0]\n# n_test  = X_test.shape[0]\n\n# oof_lgb = np.zeros((n_train, n_classes))\n# oof_xgb = np.zeros((n_train, n_classes))\n# oof_cat = np.zeros((n_train, n_classes))\n\n# test_lgb = np.zeros((n_test,  n_classes))\n# test_xgb = np.zeros((n_test,  n_classes))\n# test_cat = np.zeros((n_test,  n_classes))\n\n# # ==== 3. Generate OOF Predictions for Base Learners ====\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# for train_idx, val_idx in skf.split(X, y):\n#     X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n#     y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n#     # LightGBM\n#     m_lgb = lgb.LGBMClassifier()\n#     m_lgb.fit(X_tr, y_tr)\n#     oof_lgb[val_idx] = m_lgb.predict_proba(X_val)\n#     test_lgb += m_lgb.predict_proba(X_test) / skf.n_splits\n    \n#     # XGBoost\n#     m_xgb = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n#     m_xgb.fit(X_tr, y_tr)\n#     oof_xgb[val_idx] = m_xgb.predict_proba(X_val)\n#     test_xgb += m_xgb.predict_proba(X_test) / skf.n_splits\n    \n#     # CatBoost\n#     m_cat = cb.CatBoostClassifier(verbose=0)\n#     m_cat.fit(X_tr, y_tr)\n#     oof_cat[val_idx] = m_cat.predict_proba(X_val)\n#     test_cat += m_cat.predict_proba(X_test) / skf.n_splits\n\n# # Stack OOF predictions as new features\n# X_meta_train = np.hstack([oof_lgb, oof_xgb, oof_cat])\n# X_meta_test  = np.hstack([test_lgb, test_xgb, test_cat])\n\n# # ==== 4. Train Meta‑Model on OOF Predictions ====\n# meta_model = LogisticRegression(multi_class='multinomial', max_iter=1000)\n# meta_model.fit(X_meta_train, y)\n\n# # ==== 5. Predict with Meta‑Model and Prepare Submission ====\n# meta_preds = meta_model.predict_proba(X_meta_test)\n# top3 = np.argsort(meta_preds, axis=1)[:, -3:][:, ::-1]\n\n# submission = pd.DataFrame({\n#     'id': test_ids,\n#     'Fertilizer Name': [' '.join(le_target.inverse_transform(row)) for row in top3]\n# })\n# submission.to_csv('submission_stacked.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# with normal","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.preprocessing import LabelEncoder\n# import lightgbm as lgb\n# import xgboost as xgb\n# import catboost as cb\n\n# # Load data\n# df_train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\n# df_test = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\n# # Fix typo\n# df_train.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n# df_test.rename(columns={'Temparature': 'Temperature'}, inplace=True)\n\n# # Target Encoding\n# soil_map = df_train.groupby('Soil Type')['Fertilizer Name'].agg(lambda x: x.value_counts().idxmax())\n# crop_map = df_train.groupby('Crop Type')['Fertilizer Name'].agg(lambda x: x.value_counts().idxmax())\n# df_train['Soil_TE'] = df_train['Soil Type'].map(soil_map)\n# df_test['Soil_TE'] = df_test['Soil Type'].map(soil_map)\n# df_train['Crop_TE'] = df_train['Crop Type'].map(crop_map)\n# df_test['Crop_TE'] = df_test['Crop Type'].map(crop_map)\n\n# # Label Encoding\n# le_soil = LabelEncoder()\n# le_crop = LabelEncoder()\n# le_target = LabelEncoder()\n# df_train['Soil'] = le_soil.fit_transform(df_train['Soil Type'])\n# df_test['Soil'] = le_soil.transform(df_test['Soil Type'])\n# df_train['Crop'] = le_crop.fit_transform(df_train['Crop Type'])\n# df_test['Crop'] = le_crop.transform(df_test['Crop Type'])\n# df_train['Target'] = le_target.fit_transform(df_train['Fertilizer Name'])\n\n# # Features\n# features = ['Temperature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous', 'Soil', 'Crop']\n# X = df_train[features]\n# y = df_train['Target']\n# X_test = df_test[features]\n\n# # Predictions\n# lgb_preds = np.zeros((X_test.shape[0], len(le_target.classes_)))\n# xgb_preds = np.zeros((X_test.shape[0], len(le_target.classes_)))\n# cat_preds = np.zeros((X_test.shape[0], len(le_target.classes_)))\n\n# # Cross-validation\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# for train_idx, val_idx in skf.split(X, y):\n#     X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n#     y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n#     # LightGBM\n#     model_lgb = lgb.LGBMClassifier()\n#     model_lgb.fit(X_tr, y_tr)\n#     lgb_preds += model_lgb.predict_proba(X_test) / skf.n_splits\n\n#     # XGBoost\n#     model_xgb = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n#     model_xgb.fit(X_tr, y_tr)\n#     xgb_preds += model_xgb.predict_proba(X_test) / skf.n_splits\n\n#     # CatBoost\n#     model_cat = cb.CatBoostClassifier(verbose=0)\n#     model_cat.fit(X_tr, y_tr)\n#     cat_preds += model_cat.predict_proba(X_test) / skf.n_splits\n\n# # Ensemble\n# avg_preds = (lgb_preds + xgb_preds + cat_preds) / 3\n# top_3 = np.argsort(avg_preds, axis=1)[:, -3:][:, ::-1]\n\n# # Submission\n# submission = pd.DataFrame()\n# submission['id'] = df_test['id']\n# submission['Fertilizer Name'] = [' '.join(le_target.inverse_transform(row)) for row in top_3]\n# submission.to_csv('submission.csv', index=False)\n# submission.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model ZOO with DF_train","metadata":{}},{"cell_type":"code","source":"# # 📦 Import Libraries\n# import pandas as pd\n# import numpy as np\n# from sklearn.preprocessing import PolynomialFeatures, LabelEncoder\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.metrics import log_loss\n# import lightgbm as lgb\n# from xgboost import XGBClassifier\n# from catboost import CatBoostClassifier\n\n# # 📁 Load Data\n# df_train = pd.read_csv(\"/kaggle/input/playground-series-s5e6/train.csv\")\n# df_test  = pd.read_csv(\"/kaggle/input/playground-series-s5e6/test.csv\")\n\n# # 🧹 Drop ID column and separate target\n# df_train = df_train.drop(columns=['id'])\n# y = df_train['Fertilizer Name']\n# df_train = df_train.drop(columns=['Fertilizer Name'])\n# test_ids = df_test['id']\n# df_test  = df_test.drop(columns=['id'])\n\n# # 🔠 Label‑encode categorical features\n# for col in ['Soil Type', 'Crop Type']:\n#     le = LabelEncoder()\n#     df_train[col] = le.fit_transform(df_train[col])\n#     df_test[col]  = le.transform(df_test[col])\n\n# # 🔠 Encode target\n# target_le = LabelEncoder()\n# y_enc = target_le.fit_transform(y)\n\n# # ⚙️ Create interaction feature: Soil_Crop\n# df_train['Soil_Crop'] = df_train['Soil Type'].astype(str) + \"_\" + df_train['Crop Type'].astype(str)\n# df_test['Soil_Crop']  = df_test['Soil Type'].astype(str)  + \"_\" + df_test['Crop Type'].astype(str)\n\n# # ➕ Polynomial Features\n# num_cols = ['Temparature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous']\n# poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n# df_train_poly = pd.DataFrame(poly.fit_transform(df_train[num_cols]),\n#                              columns=poly.get_feature_names_out(num_cols),\n#                              index=df_train.index)\n# df_test_poly  = pd.DataFrame(poly.transform(df_test[num_cols]),\n#                              columns=poly.get_feature_names_out(num_cols),\n#                              index=df_test.index)\n\n# # 📈 Group‑based mean stats by Soil_Crop\n# group_stats = df_train.groupby('Soil_Crop')[num_cols].mean().add_prefix('SC_mean_')\n# df_train = df_train.join(group_stats, on='Soil_Crop')\n# df_test  = df_test.join(group_stats, on='Soil_Crop')\n\n# # 🧱 Final feature sets (drop the text interaction column, merge polys)\n# df_train_final = pd.concat([df_train.drop(columns=['Soil_Crop']), df_train_poly], axis=1)\n# df_test_final  = pd.concat([df_test.drop(columns=['Soil_Crop']), df_test_poly],  axis=1)\n\n# #to prevent duplicate columns\n# df_train_final = df_train_final.loc[:, ~df_train_final.columns.duplicated()]\n# df_test_final  = df_test_final.loc[:, ~df_test_final.columns.duplicated()]\n\n# # ⚒️ Define models (no TabNet)\n# models = {\n#     'LightGBM': lgb.LGBMClassifier(\n#         objective='multiclass', num_class=len(target_le.classes_),\n#         learning_rate=0.05, num_leaves=31, subsample=0.8,\n#         colsample_bytree=0.8, n_estimators=300\n#     ),\n#     'XGBoost': XGBClassifier(\n#         objective='multi:softprob', num_class=len(target_le.classes_),\n#         learning_rate=0.1, max_depth=6, subsample=0.8,\n#         colsample_bytree=0.8, n_estimators=300,\n#         use_label_encoder=False, eval_metric='mlogloss'\n#     ),\n#     'CatBoost': CatBoostClassifier(\n#         loss_function='MultiClass', learning_rate=0.1,\n#         depth=6, iterations=300, verbose=0\n#     )\n# }\n\n# # 🔁 Cross‑validation\n# cv_scores = {name: [] for name in models}\n# kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# for fold, (train_idx, val_idx) in enumerate(kf.split(df_train_final, y_enc), 1):\n#     X_tr, X_val = df_train_final.iloc[train_idx], df_train_final.iloc[val_idx]\n#     y_tr, y_val = y_enc[train_idx], y_enc[val_idx]\n\n#     print(f\"\\n🧪 Fold {fold}\")\n#     for name, model in models.items():\n#         print(f\"▶ Training {name}…\", end=' ')\n#         model.fit(X_tr, y_tr)\n#         preds = model.predict_proba(X_val)\n#         loss = log_loss(y_val, preds)\n#         cv_scores[name].append(loss)\n#         print(f\"LogLoss = {loss:.4f}\")\n\n# # 📊 CV Summary\n# print(\"\\n🔚 Cross‑Validation Results:\")\n# for name, scores in cv_scores.items():\n#     print(f\"{name:>8}: Mean = {np.mean(scores):.4f} ± {np.std(scores):.4f}\")\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# # ▶ Final training & submission (example with XGBoost here)\n\n# best_model = models['XGBoost']\n# best_model.fit(df_train_final, y_enc)\n\n# probs_test = best_model.predict_proba(df_test_final)\n# top3_idx   = np.argsort(probs_test, axis=1)[:, -3:][:, ::-1]\n# top3_lab   = target_le.inverse_transform(top3_idx.ravel()).reshape(top3_idx.shape)\n\n# submission = pd.DataFrame({\n#     'id': test_ids,\n#     'Fertilizer Name': [' '.join(row) for row in top3_lab]\n# })\n# submission.to_csv('submission.csv', index=False)\n# print(\"✅ submission.csv saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Light XGB model continue","metadata":{}},{"cell_type":"code","source":"# # 1. Drop the ID column\n# df_train = df_train.drop(columns=['id'])\n# df_test  = df_test .drop(columns=['id'])\n\n# # 2. One‑hot encode Soil Type & Crop Type\n# df_train = pd.get_dummies(df_train, columns=['Soil Type', 'Crop Type'])\n# df_test  = pd.get_dummies(df_test,  columns=['Soil Type', 'Crop Type'])\n\n# # Align train/test columns (in case some categories appear only in one set)\n# df_test = df_test.reindex(columns=df_train.columns.drop('Fertilizer Name'), fill_value=0)\n\n# # 3. Encode the target variable\n# le_target = LabelEncoder()\n# df_train['Fertilizer Label'] = le_target.fit_transform(df_train['Fertilizer Name'])\n\n# # Now drop the original target string column\n# df_train = df_train.drop(columns=['Fertilizer Name'])\n\n# # 4. (Optional) Scale numeric features if you plan to use non-tree models\n# # from sklearn.preprocessing import StandardScaler\n# # num_cols = ['Temperature','Humidity','Moisture','Nitrogen','Potassium','Phosphorous']\n# # scaler = StandardScaler()\n# # df_train[num_cols] = scaler.fit_transform(df_train[num_cols])\n# # df_test[num_cols]  = scaler.transform(df_test[num_cols])\n\n# # Final feature/label splits\n# X = df_train.drop(columns=['Fertilizer Label'])\n# y = df_train['Fertilizer Label']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"X shape:\", X.shape)\n# print(\"y shape:\", y.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pd.Series(y).value_counts(normalize=True).head(10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LGBM model","metadata":{}},{"cell_type":"code","source":"# import lightgbm as lgb\n# from sklearn.model_selection import train_test_split\n\n# # 1. Split the data\n# X_train, X_val, y_train, y_val = train_test_split(\n#     X, y, test_size=0.2, random_state=42, stratify=y\n# )\n\n# # 2. Convert to LightGBM datasets\n# train_data = lgb.Dataset(X_train, label=y_train)\n# val_data   = lgb.Dataset(X_val, label=y_val)\n\n# # 3. Define parameters\n# params = {\n#     'objective': 'multiclass',\n#     'num_class': y.nunique(),\n#     'metric': 'multi_logloss',\n#     'learning_rate': 0.1,\n#     'verbosity': -1\n# }\n\n# # 4. Train with callbacks for early stopping\n# model = lgb.train(\n#     params,\n#     train_data,\n#     valid_sets=[val_data],                      # Must pass validation set\n#     callbacks=[\n#         lgb.early_stopping(stopping_rounds=20),\n#         lgb.log_evaluation(period=10)           # Show logs every 10 rounds\n#     ],\n#     num_boost_round=500                         # Max rounds\n# )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 1: Predict probabilities for each fertilizer class\n# probs = model.predict(df_test)\n\n# # Step 2: Get top 3 class indices with highest probability\n# top3 = np.argsort(probs, axis=1)[:, -3:][:, ::-1]  # [:, ::-1] reverses to get descending order\n\n# # Step 3: Convert numeric predictions back to original fertilizer names\n# top3_labels = le_target.inverse_transform(top3.ravel()).reshape(top3.shape)\n\n# # Step 4: Load original test file to get 'id' column\n# df_test_original = pd.read_csv(\"/kaggle/input/playground-series-s5e6/test.csv\")  # use your path\n\n# # Step 5: Create submission DataFrame\n# submission = pd.DataFrame({\n#     'id': df_test_original['id'],\n#     'Fertilizer Name': [' '.join(row) for row in top3_labels]\n# })\n\n# # Step 6: Save to CSV in correct format\n# submission.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# New model","metadata":{}},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import lightgbm as lgb\n\n# # Plot feature importance\n# lgb.plot_importance(model, max_num_features=15, importance_type='gain', figsize=(10, 6))\n# plt.title(\"Top 15 Important Features\")\n# plt.grid(True)\n# plt.tight_layout()\n# plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from xgboost import XGBClassifier\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.metrics import log_loss\n# import numpy as np\n\n# # Encode target labels again (just in case)\n# le = LabelEncoder()\n# y_encoded = le.fit_transform(y)\n\n# # Train/val split\n# X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n\n# # Train XGBoost\n# xgb_model = XGBClassifier(\n#     objective='multi:softprob',\n#     num_class=len(le.classes_),\n#     learning_rate=0.1,\n#     max_depth=6,\n#     n_estimators=300,\n#     subsample=0.8,\n#     colsample_bytree=0.8,\n#     use_label_encoder=False,\n#     eval_metric='mlogloss'\n# )\n# xgb_model.fit(X_train, y_train)\n\n# # Predict and generate top-3\n# xgb_probs = xgb_model.predict_proba(df_test)\n# top3_xgb = np.argsort(xgb_probs, axis=1)[:, -3:][:, ::-1]\n# top3_xgb_labels = le.inverse_transform(top3_xgb.ravel()).reshape(top3_xgb.shape)\n\n# # Prepare submission\n# submission_xgb = pd.DataFrame({\n#     'id': df_test_original['id'],\n#     'Fertilizer Name': [' '.join(row) for row in top3_xgb_labels]\n# })\n# submission_xgb.to_csv('submission_xgb.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# params = {\n#     'objective': 'multiclass',\n#     'num_class': y.nunique(),\n#     'metric': 'multi_logloss',\n#     'learning_rate': 0.05,\n#     'max_depth': 8,\n#     'num_leaves': 31,\n#     'subsample': 0.8,\n#     'colsample_bytree': 0.8,\n#     'verbosity': -1\n# }\n# # You can now train like before using this improved param set.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import optuna\n\n# def objective(trial):\n#     param = {\n#         'objective': 'multiclass',\n#         'num_class': y.nunique(),\n#         'metric': 'multi_logloss',\n#         'learning_rate': trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n#         'num_leaves': trial.suggest_int(\"num_leaves\", 20, 100),\n#         'max_depth': trial.suggest_int(\"max_depth\", 3, 15),\n#         'subsample': trial.suggest_float(\"subsample\", 0.5, 1.0),\n#         'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n#         'verbosity': -1\n#     }\n\n#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n#     dtrain = lgb.Dataset(X_train, label=y_train)\n#     dval = lgb.Dataset(X_val, label=y_val)\n\n#     model = lgb.train(param, dtrain, valid_sets=[dval], callbacks=[lgb.early_stopping(10)], num_boost_round=300)\n#     preds = model.predict(X_val)\n#     return log_loss(y_val, preds)\n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=30)\n\n# # Best params\n# print(\"Best parameters: \", study.best_params)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# New XGB code","metadata":{}},{"cell_type":"code","source":"# '''\n# XGBoost Pipeline for Fertilizer Recommendation (Kaggle Playground S5E6)\n# Step-by-step code with detailed comments\n# '''\n\n# # STEP 1: Import necessary libraries\n# import pandas as pd\n# import numpy as np\n# import matplotlib.pyplot as plt\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.metrics import log_loss\n# from xgboost import XGBClassifier\n\n# # STEP 2: Load the data\n# # Replace the paths below with your actual Kaggle dataset paths\n# train_path = '/kaggle/input/playground-series-s5e6/train.csv'\n# test_path  = '/kaggle/input/playground-series-s5e6/test.csv'\n\n# df_train = pd.read_csv(train_path)\n# df_test  = pd.read_csv(test_path)\n\n# # Quick preview\n# print(df_train.head())\n# print(df_train.info())\n# print(\"Unique fertilizers:\", df_train['Fertilizer Name'].nunique())\n\n# # STEP 3: Basic preprocessing\n# # 3.1 Drop 'id' columns (not predictive)\n# df_train = df_train.drop(columns=['id'])\n# df_test_ids = df_test['id'].copy()\n# df_test  = df_test.drop(columns=['id'])\n\n# # 3.2 Create interaction feature: Soil_Crop\n# # This captures joint effect of soil and crop combination\n# for df in [df_train, df_test]:\n#     df['Soil_Crop'] = df['Soil Type'] + '_' + df['Crop Type']\n\n# # STEP 4: Encoding categorical features\n# # Use LabelEncoder for simplicity on all categorical predictors\n# cat_cols = ['Soil Type', 'Crop Type', 'Soil_Crop']\n# le_dict = {}\n# for col in cat_cols:\n#     le = LabelEncoder()\n#     df_train[col] = le.fit_transform(df_train[col])\n#     df_test[col]  = le.transform(df_test[col])\n#     le_dict[col] = le  # store encoder if needed later\n\n# # Encode the target variable\n# le_target = LabelEncoder()\n# train_labels = le_target.fit_transform(df_train['Fertilizer Name'])\n# # Drop original target column\n# df_train = df_train.drop(columns=['Fertilizer Name'])\n\n# # STEP 5: Prepare feature matrix X and label vector y\n# X = df_train.copy()\n# y = train_labels\n# X_test = df_test.copy()\n\n# # STEP 6: Train-validation split for local evaluation\n# X_train, X_val, y_train, y_val = train_test_split(\n#     X, y, test_size=0.2, random_state=42, stratify=y\n# )\n\n# # STEP 7: Train XGBoost model with early stopping\n# xgb_model = XGBClassifier(\n#     objective='multi:softprob',       # multiclass classification with probabilities\n#     num_class=len(le_target.classes_),\n#     learning_rate=0.1,                # step size shrinkage\n#     max_depth=6,                      # maximum tree depth\n#     n_estimators=300,                 # number of trees\n#     subsample=0.8,                    # subsample ratio of training instances\n#     colsample_bytree=0.8,             # subsample ratio of columns when constructing each tree\n#     use_label_encoder=False,          # disable default label encoder\n#     eval_metric='mlogloss',           # evaluation metric\n#     random_state=42\n# )\n\n# # Fit with early stopping on validation data\n# xgb_model.fit(\n#     X_train, y_train,\n#     eval_set=[(X_val, y_val)],\n#     early_stopping_rounds=20,\n#     verbose=True\n# )\n\n# # STEP 8: Plot feature importance\n# plt.figure(figsize=(10, 6))\n# importance = xgb_model.feature_importances_\n# features = X.columns\n# indices = np.argsort(importance)[-15:]  # top 15 features\n# plt.barh(range(len(indices)), importance[indices], align='center')\n# plt.yticks(range(len(indices)), [features[i] for i in indices])\n# plt.title('Top 15 XGBoost Feature Importances')\n# plt.xlabel('Feature importance')\n# plt.tight_layout()\n# plt.show()\n\n# # STEP 9: Generate top-3 predictions for test set\n# # 9.1 Predict class probabilities\n# probs = xgb_model.predict_proba(X_test)\n# # 9.2 Select top 3 indices per sample\n# top3_indices = np.argsort(probs, axis=1)[:, -3:][:, ::-1]\n# # 9.3 Map indices back to fertilizer names\n# top3_labels = le_target.inverse_transform(top3_indices.ravel()).reshape(top3_indices.shape)\n\n# # STEP 10: Prepare and save submission\n# submission = pd.DataFrame({\n#     'id': df_test_ids,\n#     'Fertilizer Name': [' '.join(row) for row in top3_labels]\n# })\n\n# submission.to_csv('submission_xgb.csv', index=False)\n# print(\"Submission file 'submission_xgb.csv' created.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}